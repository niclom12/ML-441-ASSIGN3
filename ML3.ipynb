{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hGluL50RhGX"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJQCXSL3WZpP"
      },
      "outputs": [],
      "source": [
        "path = kagglehub.dataset_download(\"sumanthvrao/daily-climate-time-series-data\")\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJ0h8rYoX3K9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "print(os.listdir(path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08b93428"
      },
      "outputs": [],
      "source": [
        "train_file_name = \"DailyDelhiClimateTrain.csv\"\n",
        "test_file_name = \"DailyDelhiClimateTest.csv\"\n",
        "\n",
        "train_file_path = os.path.join(path, train_file_name)\n",
        "test_file_path = os.path.join(path, test_file_name)\n",
        "\n",
        "train_df = pd.read_csv(train_file_path)\n",
        "test_df = pd.read_csv(test_file_path)\n",
        "\n",
        "print(\"Train DataFrame head:\")\n",
        "display(train_df.head())\n",
        "print(\"\\nTest DataFrame head:\")\n",
        "display(test_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import math\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "\n",
        "train_file_name = \"DailyDelhiClimateTrain.csv\"\n",
        "test_file_name = \"DailyDelhiClimateTest.csv\"\n",
        "\n",
        "TRAIN_CSV_PATH = os.path.join(path, train_file_name)\n",
        "TEST_CSV_PATH = os.path.join(path, test_file_name)\n",
        "\n",
        "TIME_COLUMN = \"date\"\n",
        "TARGET_COLUMN = \"meantemp\"\n",
        "FEATURE_COLUMNS = [\"humidity\", \"wind_speed\", \"meanpressure\"]\n",
        "\n",
        "\n",
        "HYPERPARAMETERS_TO_TEST = {\n",
        "    'lookback': [14, 30],\n",
        "    'hidden_units': [32],\n",
        "}\n",
        "N_EPOCHS = 50\n",
        "PATIENCE = 10\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "N_CV_SPLITS = 3\n",
        "VALIDATION_BLOCK_SIZE = 90 #fold will be 90 days long.\n",
        "\n",
        "\n",
        "def load_data(train_path, test_path, time_col):\n",
        "    train_df = pd.read_csv(train_path, parse_dates=[time_col], index_col=time_col)\n",
        "    test_df = pd.read_csv(test_path, parse_dates=[time_col], index_col=time_col)\n",
        "    return train_df.sort_index(), test_df.sort_index()\n",
        "\n",
        "def create_windows(data, lookback, target_column_index):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - lookback):\n",
        "        X.append(data[i:(i + lookback)])\n",
        "        y.append(data[i + lookback, target_column_index])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def get_cv_splits(data_length, n_splits, val_size):\n",
        "    initial_train_size = data_length - (n_splits * val_size)\n",
        "    for i in range(n_splits):\n",
        "        train_end = initial_train_size + i * val_size\n",
        "        val_end = train_end + val_size\n",
        "\n",
        "        train_indices = range(0, train_end)\n",
        "        val_indices = range(train_end, val_end)\n",
        "        yield train_indices, val_indices\n",
        "\n",
        "\n",
        "\n",
        "def build_elman_model(input_shape, hidden_units):\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=input_shape),\n",
        "        layers.SimpleRNN(hidden_units, activation='tanh'),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_multi_layer_model(input_shape, hidden_units):\n",
        "    \"\"\"Builds a multi-layer RNN (two stacked SimpleRNN layers).\"\"\"\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=input_shape),\n",
        "        layers.SimpleRNN(hidden_units, activation='tanh', return_sequences=True),\n",
        "        layers.SimpleRNN(hidden_units, activation='tanh'),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "class JordanRNN(keras.Model):\n",
        "    def __init__(self, hidden_units):\n",
        "        super().__init__()\n",
        "        self.rnn_cell = layers.SimpleRNNCell(hidden_units, activation='tanh')\n",
        "        # The final output layer.\n",
        "        self.dense_output = layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        hidden_state = tf.zeros((batch_size, self.rnn_cell.units))\n",
        "        previous_output = tf.zeros((batch_size, 1))\n",
        "\n",
        "        for t in range(inputs.shape[1]):\n",
        "            current_features = inputs[:, t, :]\n",
        "            # Combine current features with the previous output.\n",
        "            combined_input = tf.concat([current_features, previous_output], axis=1)\n",
        "            _, [hidden_state] = self.rnn_cell(combined_input, [hidden_state])\n",
        "\n",
        "            # Calculate the output used in the next iteration.\n",
        "            previous_output = self.dense_output(hidden_state)\n",
        "\n",
        "        return previous_output\n",
        "\n",
        "\n",
        "def main():\n",
        "    train_df, test_df = load_data(TRAIN_CSV_PATH, TEST_CSV_PATH, TIME_COLUMN)\n",
        "    train_df = train_df.ffill()\n",
        "    test_df = test_df.ffill()\n",
        "\n",
        "    models_to_evaluate = {\n",
        "        'Elman': build_elman_model,\n",
        "        'Multi-Layer': build_multi_layer_model,\n",
        "        'Jordan': JordanRNN\n",
        "    }\n",
        "\n",
        "    final_results = {}\n",
        "\n",
        "    for model_name, model_builder in models_to_evaluate.items():\n",
        "        best_params = {}\n",
        "        best_avg_rmse = float('inf')\n",
        "        for lookback in HYPERPARAMETERS_TO_TEST['lookback']:\n",
        "            for hidden_units in HYPERPARAMETERS_TO_TEST['hidden_units']:\n",
        "                print(f\"  Testing params: lookback={lookback}, hidden_units={hidden_units}\")\n",
        "                fold_rmses = []\n",
        "\n",
        "                cv_splits = get_cv_splits(len(train_df), N_CV_SPLITS, VALIDATION_BLOCK_SIZE)\n",
        "\n",
        "                for train_idx, val_idx in cv_splits:\n",
        "                    train_fold = train_df.iloc[train_idx]\n",
        "                    val_fold = train_df.iloc[val_idx]\n",
        "\n",
        "                    all_cols = [TARGET_COLUMN] + FEATURE_COLUMNS\n",
        "\n",
        "                    scaler_X = StandardScaler()\n",
        "                    scaler_y = StandardScaler()\n",
        "\n",
        "                    train_features_scaled = scaler_X.fit_transform(train_fold[FEATURE_COLUMNS])\n",
        "                    train_target_scaled = scaler_y.fit_transform(train_fold[[TARGET_COLUMN]])\n",
        "\n",
        "                    val_features_scaled = scaler_X.transform(val_fold[FEATURE_COLUMNS])\n",
        "                    val_target_scaled = scaler_y.transform(val_fold[[TARGET_COLUMN]])\n",
        "\n",
        "                    train_scaled_data = np.hstack([train_target_scaled, train_features_scaled])\n",
        "\n",
        "                    validation_context = np.vstack([\n",
        "                        train_scaled_data[-lookback:],\n",
        "                        np.hstack([val_target_scaled, val_features_scaled])\n",
        "                    ])\n",
        "\n",
        "                    X_train, y_train = create_windows(train_scaled_data, lookback, target_column_index=0)\n",
        "                    X_val, y_val = create_windows(validation_context, lookback, target_column_index=0)\n",
        "\n",
        "                    if X_train.shape[0] == 0 or X_val.shape[0] == 0:\n",
        "                        continue # Skip if a not enough data\n",
        "\n",
        "                    n_features = X_train.shape[2]\n",
        "                    input_shape = (lookback, n_features)\n",
        "\n",
        "                    if model_name == 'Jordan':\n",
        "                        model = model_builder(hidden_units=hidden_units)\n",
        "                    else:\n",
        "                        model = model_builder(input_shape=input_shape, hidden_units=hidden_units)\n",
        "\n",
        "                    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "                    early_stopping = keras.callbacks.EarlyStopping(\n",
        "                        monitor='val_loss', patience=PATIENCE, restore_best_weights=True\n",
        "                    )\n",
        "\n",
        "                    model.fit(X_train, y_train,\n",
        "                              validation_data=(X_val, y_val),\n",
        "                              epochs=N_EPOCHS,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              callbacks=[early_stopping],\n",
        "                              verbose=0)\n",
        "\n",
        "                    predictions_scaled = model.predict(X_val, verbose=0)\n",
        "                    predictions_unscaled = scaler_y.inverse_transform(predictions_scaled)\n",
        "                    true_values_unscaled = scaler_y.inverse_transform(y_val.reshape(-1, 1))\n",
        "\n",
        "                    fold_rmse = math.sqrt(mean_squared_error(true_values_unscaled, predictions_unscaled))\n",
        "                    fold_rmses.append(fold_rmse)\n",
        "\n",
        "                avg_rmse = np.mean(fold_rmses)\n",
        "                print(f\"Average CV RMSE: {avg_rmse:.4f}\")\n",
        "                if avg_rmse < best_avg_rmse:\n",
        "                    best_avg_rmse = avg_rmse\n",
        "                    best_params = {'lookback': lookback, 'hidden_units': hidden_units}\n",
        "\n",
        "        print(f\"\\n  Best parameters found for {model_name}: {best_params} (RMSE: {best_avg_rmse:.4f})\")\n",
        "\n",
        "\n",
        "        final_lookback = best_params['lookback']\n",
        "        final_hidden_units = best_params['hidden_units']\n",
        "\n",
        "        scaler_X_final = StandardScaler()\n",
        "        scaler_y_final = StandardScaler()\n",
        "\n",
        "        train_features_scaled_final = scaler_X_final.fit_transform(train_df[FEATURE_COLUMNS])\n",
        "        train_target_scaled_final = scaler_y_final.fit_transform(train_df[[TARGET_COLUMN]])\n",
        "\n",
        "        test_features_scaled_final = scaler_X_final.transform(test_df[FEATURE_COLUMNS])\n",
        "        test_target_scaled_final = scaler_y_final.transform(test_df[[TARGET_COLUMN]])\n",
        "\n",
        "        # Combine data for windowing\n",
        "        train_scaled_final = np.hstack([train_target_scaled_final, train_features_scaled_final])\n",
        "\n",
        "        test_context_final = np.vstack([\n",
        "            train_scaled_final[-final_lookback:],\n",
        "            np.hstack([test_target_scaled_final, test_features_scaled_final])\n",
        "        ])\n",
        "\n",
        "        X_train_final, y_train_final = create_windows(train_scaled_final, final_lookback, 0)\n",
        "        X_test_final, y_test_final = create_windows(test_context_final, final_lookback, 0)\n",
        "\n",
        "        n_features_final = X_train_final.shape[2]\n",
        "        input_shape_final = (final_lookback, n_features_final)\n",
        "\n",
        "        if model_name == 'Jordan':\n",
        "            final_model = model_builder(hidden_units=final_hidden_units)\n",
        "        else:\n",
        "            final_model = model_builder(input_shape=input_shape_final, hidden_units=final_hidden_units)\n",
        "\n",
        "        final_model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "        final_model.fit(X_train_final, y_train_final,\n",
        "                        epochs=N_EPOCHS,\n",
        "                        batch_size=BATCH_SIZE,\n",
        "                        verbose=0)\n",
        "\n",
        "        final_predictions_scaled = final_model.predict(X_test_final, verbose=0)\n",
        "        final_predictions_unscaled = scaler_y_final.inverse_transform(final_predictions_scaled)\n",
        "        final_true_values_unscaled = scaler_y_final.inverse_transform(y_test_final.reshape(-1, 1))\n",
        "\n",
        "        final_test_rmse = math.sqrt(mean_squared_error(final_true_values_unscaled, final_predictions_unscaled))\n",
        "        print(f\"  -> Final Test RMSE for {model_name}: {final_test_rmse:.4f}\")\n",
        "\n",
        "        final_results[model_name] = {\n",
        "            'best_params': best_params,\n",
        "            'test_rmse': final_test_rmse,\n",
        "            'predictions': final_predictions_unscaled.flatten(),\n",
        "            'true_values': final_true_values_unscaled.flatten()\n",
        "        }\n",
        "\n",
        "    print(f\"\\n{'='*30}\\nFINAL RESULTS SUMMARY\\n{'='*30}\")\n",
        "    best_overall_model = None\n",
        "    lowest_rmse = float('inf')\n",
        "\n",
        "    for model_name, result in final_results.items():\n",
        "        print(f\"Model: {model_name}\")\n",
        "        print(f\"  - Best Hyperparameters: {result['best_params']}\")\n",
        "        print(f\"  - Final Test RMSE: {result['test_rmse']:.4f}\")\n",
        "        if result['test_rmse'] < lowest_rmse:\n",
        "            lowest_rmse = result['test_rmse']\n",
        "            best_overall_model = model_name\n",
        "\n",
        "    print(f\"\\n Overall Best Performing Model: {best_overall_model} (RMSE: {lowest_rmse:.4f})\")\n",
        "\n",
        "main()\n",
        "\n",
        "# Commenrt on the custom jordan implementation\n",
        "# Comment on the steps taken to prevent overfitting\n",
        "# Describe Cross validation approach used\n",
        "# Describe data preprocessing and justifications\n",
        "# Describe optimiser and loss function used\n",
        "# Carefully define the empirical process that you have followed, and describe this process in your report.The process has to include settings for all hyperparameters, neural network architecture, performance measures, and the process followed to determine which simple recurrent neural network performed best for each of the datasets."
      ],
      "metadata": {
        "id": "qNF70SJVNorb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from statsmodels.tsa.stattools import adfuller, kpss\n",
        "\n",
        "def adf_test(series, signif=0.05):\n",
        "    result = adfuller(series.dropna(), autolag='AIC')\n",
        "    stat, pval, usedlag, nobs, crit_values, icbest = result\n",
        "    stationary = pval < signif\n",
        "    return {'stat': stat, 'pval': pval, 'usedlag': usedlag, 'nobs': nobs,\n",
        "            'crit': crit_values, 'stationary': stationary}\n",
        "\n",
        "def kpss_test(series, regression='c', signif=0.05):\n",
        "    stat, pval, nlags, crit_values = kpss(series.dropna(), regression=regression, nlags=\"auto\")\n",
        "    stationary = pval > signif\n",
        "    return {'stat': stat, 'pval': pval, 'nlags': nlags, 'crit': crit_values, 'stationary': stationary}\n",
        "\n",
        "def check_stationarity(df, columns=None, signif=0.05, verbose=True):\n",
        "    df_copy = df.copy()\n",
        "    if not isinstance(df_copy.index, pd.DatetimeIndex):\n",
        "        for alt in ['date', 'time', 'timestamp']:\n",
        "            if alt in df_copy.columns:\n",
        "                df_copy[alt] = pd.to_datetime(df_copy[alt], errors='coerce')\n",
        "                df_copy = df_copy.set_index(alt)\n",
        "                break\n",
        "\n",
        "    if columns is None:\n",
        "        columns = df_copy.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "    results = {}\n",
        "    for col in columns:\n",
        "        series = df_copy[col]\n",
        "        adf_res = adf_test(series, signif)\n",
        "        kpss_res = kpss_test(series, signif=signif)\n",
        "        if adf_res['stationary'] and kpss_res['stationary']:\n",
        "            conclusion = 'Stationary'\n",
        "        elif not adf_res['stationary'] and not kpss_res['stationary']:\n",
        "            conclusion = 'Non-stationary'\n",
        "        else:\n",
        "            conclusion = 'Mixed/Borderline'\n",
        "        results[col] = {'ADF': adf_res, 'KPSS': kpss_res, 'conclusion': conclusion}\n",
        "        if verbose:\n",
        "            print(f\"\\nColumn: {col}\")\n",
        "            print(f\"  ADF: stat={adf_res['stat']:.4f}, p={adf_res['pval']:.4f}, stationary={adf_res['stationary']}\")\n",
        "            print(f\"  KPSS: stat={kpss_res['stat']:.4f}, p={kpss_res['pval']:.4f}, stationary={kpss_res['stationary']}\")\n",
        "            print(f\"  => Overall conclusion: {conclusion}\")\n",
        "    return results\n",
        "\n",
        "\n",
        "results = check_stationarity(train_df)\n"
      ],
      "metadata": {
        "id": "bNRYMIDHnaic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = kagglehub.dataset_download(\"gabrielsantello/airline-baggage-complaints-time-series-dataset\")\n",
        "file_name = \"baggagecomplaints.csv\"\n",
        "file_path = os.path.join(path, file_name)\n",
        "df = pd.read_csv(file_path)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "8aMRXC_9r1t5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns.tolist())"
      ],
      "metadata": {
        "id": "pelztNoBtZcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "FILE_PATH = os.path.join(path, file_name)\n",
        "TARGET_COLUMN = \"baggage\"\n",
        "NUMERIC_FEATURES = [\"scheduled\", \"cancelled\", \"enplaned\"]\n",
        "CATEGORICAL_FEATURE = \"airline\"\n",
        "\n",
        "HYPERPARAMETERS_TO_TEST = {\n",
        "    'lookback': [24],\n",
        "    'hidden_units': [32],\n",
        "    'learning_rate': [0.001],\n",
        "    'dropout': [0.2],\n",
        "}\n",
        "\n",
        "N_EPOCHS = 20\n",
        "PATIENCE = 10\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "TEST_SET_FRACTION = 0.2\n",
        "N_CV_SPLITS = 3\n",
        "VALIDATION_BLOCK_SIZE = 24\n",
        "\n",
        "def load_and_prepare_data(filepath):\n",
        "    df = pd.read_csv(filepath)\n",
        "    df.columns = df.columns.str.strip().str.lower()\n",
        "    df['date'] = pd.to_datetime(df['date'], format=\"%m/%Y\")\n",
        "    return df.set_index('date').sort_index()\n",
        "\n",
        "def add_time_features(df):\n",
        "    df_copy = df.copy()\n",
        "    df_copy['month'] = df_copy.index.month\n",
        "    df_copy['month_sin'] = np.sin(2 * np.pi * (df_copy['month'] - 1) / 12)\n",
        "    df_copy['month_cos'] = np.cos(2 * np.pi * (df_copy['month'] - 1) / 12)\n",
        "    df_copy['time_idx'] = np.arange(len(df_copy))\n",
        "    return df_copy\n",
        "\n",
        "def create_windows(data, lookback):\n",
        "    X, y = [], []\n",
        "    target_column_index = 0\n",
        "    for i in range(len(data) - lookback):\n",
        "        X.append(data[i:(i + lookback)])\n",
        "        y.append(data[i + lookback, target_column_index])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def get_cv_splits(data_length, n_splits, val_size):\n",
        "    initial_train_size = data_length - (n_splits * val_size)\n",
        "    for i in range(n_splits):\n",
        "        train_end = initial_train_size + i * val_size\n",
        "        val_end = train_end + val_size\n",
        "        yield range(0, train_end), range(train_end, val_end)\n",
        "\n",
        "\n",
        "def build_elman_model(input_shape, hidden_units, dropout):\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=input_shape),\n",
        "        layers.SimpleRNN(hidden_units, activation='tanh', dropout=dropout),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_multi_layer_model(input_shape, hidden_units, dropout):\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=input_shape),\n",
        "        layers.SimpleRNN(hidden_units, activation='tanh', return_sequences=True, dropout=dropout),\n",
        "        layers.SimpleRNN(hidden_units, activation='tanh', dropout=dropout),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "class JordanRNN(keras.Model):\n",
        "    def __init__(self, hidden_units):\n",
        "        super().__init__()\n",
        "        self.rnn_cell = layers.SimpleRNNCell(hidden_units, activation='tanh')\n",
        "        # The final output layer.\n",
        "        self.dense_output = layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        hidden_state = tf.zeros((batch_size, self.rnn_cell.units))\n",
        "        previous_output = tf.zeros((batch_size, 1))\n",
        "\n",
        "        for t in range(inputs.shape[1]):\n",
        "            current_features = inputs[:, t, :]\n",
        "            # Combine current features with the previous output.\n",
        "            combined_input = tf.concat([current_features, previous_output], axis=1)\n",
        "            _, [hidden_state] = self.rnn_cell(combined_input, [hidden_state])\n",
        "\n",
        "            # Calculate the output used in the next iteration.\n",
        "            previous_output = self.dense_output(hidden_state)\n",
        "\n",
        "        return previous_output\n",
        "\n",
        "\n",
        "full_df = load_and_prepare_data(FILE_PATH)\n",
        "full_df = full_df.ffill().bfill()\n",
        "\n",
        "test_size = int(len(full_df) * TEST_SET_FRACTION)\n",
        "trainval_df = full_df.iloc[:-test_size].copy()\n",
        "test_df = full_df.iloc[-test_size:].copy()\n",
        "results = check_stationarity(trainval_df)\n",
        "\n",
        "models_to_evaluate = {\n",
        "    'Elman': build_elman_model,\n",
        "    'Multi-Layer': build_multi_layer_model,\n",
        "    'Jordan': JordanRNN\n",
        "}\n",
        "param_grid = HYPERPARAMETERS_TO_TEST\n",
        "param_combos = list(itertools.product(\n",
        "    param_grid['lookback'], param_grid['hidden_units'],\n",
        "    param_grid['learning_rate'], param_grid['dropout']\n",
        "))\n",
        "\n",
        "final_results = {}\n",
        "\n",
        "for model_name, model_builder in models_to_evaluate.items():\n",
        "    print(f\"\\n{'='*40}\\nStarting evaluation for: {model_name}\\n{'='*40}\")\n",
        "\n",
        "    best_params = {}\n",
        "    best_avg_rmse = float('inf')\n",
        "\n",
        "    for lookback, hidden_units, lr, dropout in param_combos:\n",
        "        print(f\"  Testing params: lookback={lookback}, hidden={hidden_units}, lr={lr}, dropout={dropout}\")\n",
        "        fold_rmses = []\n",
        "        cv_splits = get_cv_splits(len(trainval_df), N_CV_SPLITS, VALIDATION_BLOCK_SIZE)\n",
        "\n",
        "        for train_idx, val_idx in cv_splits:\n",
        "            train_fold = trainval_df.iloc[train_idx].copy()\n",
        "            val_fold = trainval_df.iloc[val_idx].copy()\n",
        "\n",
        "            train_fold = add_time_features(train_fold)\n",
        "            val_fold = add_time_features(val_fold)\n",
        "\n",
        "            train_dummies = pd.get_dummies(train_fold[CATEGORICAL_FEATURE], prefix='air')\n",
        "            val_dummies = pd.get_dummies(val_fold[CATEGORICAL_FEATURE], prefix='air')\n",
        "            val_dummies = val_dummies.reindex(columns=train_dummies.columns, fill_value=0)\n",
        "            train_fold = pd.concat([train_fold, train_dummies], axis=1)\n",
        "            val_fold = pd.concat([val_fold, val_dummies], axis=1)\n",
        "\n",
        "            time_features = ['month_sin', 'month_cos', 'time_idx']\n",
        "            all_features = NUMERIC_FEATURES + time_features + list(train_dummies.columns)\n",
        "\n",
        "            scaler_X = StandardScaler()\n",
        "            scaler_y = StandardScaler()\n",
        "\n",
        "            train_fold[all_features] = scaler_X.fit_transform(train_fold[all_features])\n",
        "            train_fold[[TARGET_COLUMN]] = scaler_y.fit_transform(train_fold[[TARGET_COLUMN]])\n",
        "            val_fold[all_features] = scaler_X.transform(val_fold[all_features])\n",
        "            val_fold[[TARGET_COLUMN]] = scaler_y.transform(val_fold[[TARGET_COLUMN]])\n",
        "\n",
        "            train_data_for_windows = train_fold[[TARGET_COLUMN] + all_features].values\n",
        "            val_context = pd.concat([train_fold.tail(lookback), val_fold])\n",
        "            val_data_for_windows = val_context[[TARGET_COLUMN] + all_features].values\n",
        "\n",
        "            X_train, y_train = create_windows(train_data_for_windows, lookback)\n",
        "            X_val, y_val = create_windows(val_data_for_windows, lookback)\n",
        "\n",
        "            if X_train.shape[0] == 0 or X_val.shape[0] == 0: continue\n",
        "\n",
        "            input_shape = (lookback, X_train.shape[2])\n",
        "            if model_name == 'Elman':\n",
        "                model = model_builder(input_shape=input_shape, hidden_units=hidden_units, dropout=dropout)\n",
        "            elif model_name == 'Multi-Layer':\n",
        "                model = model_builder(input_shape=input_shape, hidden_units=hidden_units, dropout=dropout)\n",
        "            elif model_name == 'Jordan':\n",
        "                model = model_builder(hidden_units=hidden_units)\n",
        "                model.build(input_shape)\n",
        "\n",
        "            model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), loss='mse')\n",
        "            early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
        "\n",
        "            model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
        "                      epochs=N_EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping], verbose=0)\n",
        "\n",
        "            preds_scaled = model.predict(X_val, verbose=0)\n",
        "            preds_unscaled = scaler_y.inverse_transform(preds_scaled)\n",
        "            true_unscaled = scaler_y.inverse_transform(y_val.reshape(-1, 1))\n",
        "            fold_rmse = math.sqrt(mean_squared_error(true_unscaled, preds_unscaled))\n",
        "            fold_rmses.append(fold_rmse)\n",
        "\n",
        "        avg_rmse = np.mean(fold_rmses) if fold_rmses else float('inf')\n",
        "        if avg_rmse < best_avg_rmse:\n",
        "            best_avg_rmse = avg_rmse\n",
        "            best_params = {'lookback': lookback, 'hidden_units': hidden_units, 'lr': lr, 'dropout': dropout}\n",
        "\n",
        "    print(f\"\\n  Best parameters for {model_name}: {best_params} (Avg. CV RMSE: {best_avg_rmse:.2f})\")\n",
        "\n",
        "\n",
        "    train_final = add_time_features(trainval_df)\n",
        "    test_final = add_time_features(test_df)\n",
        "\n",
        "    train_dummies_final = pd.get_dummies(train_final[CATEGORICAL_FEATURE], prefix='air')\n",
        "    test_dummies_final = pd.get_dummies(test_final[CATEGORICAL_FEATURE], prefix='air')\n",
        "    test_dummies_final = test_dummies_final.reindex(columns=train_dummies_final.columns, fill_value=0)\n",
        "    train_final = pd.concat([train_final, train_dummies_final], axis=1)\n",
        "    test_final = pd.concat([test_final, test_dummies_final], axis=1)\n",
        "\n",
        "    time_features_final = ['month_sin', 'month_cos', 'time_idx']\n",
        "    all_features_final = NUMERIC_FEATURES + time_features_final + list(train_dummies_final.columns)\n",
        "\n",
        "    scaler_X_final = StandardScaler()\n",
        "    scaler_y_final = StandardScaler()\n",
        "\n",
        "    train_final[all_features_final] = scaler_X_final.fit_transform(train_final[all_features_final])\n",
        "    train_final[[TARGET_COLUMN]] = scaler_y_final.fit_transform(train_final[[TARGET_COLUMN]])\n",
        "    test_final[all_features_final] = scaler_X_final.transform(test_final[all_features_final])\n",
        "    test_final[[TARGET_COLUMN]] = scaler_y_final.transform(test_final[[TARGET_COLUMN]])\n",
        "\n",
        "    train_data_final = train_final[[TARGET_COLUMN] + all_features_final].values\n",
        "    test_context_final = pd.concat([train_final.tail(best_params['lookback']), test_final])\n",
        "    test_data_final = test_context_final[[TARGET_COLUMN] + all_features_final].values\n",
        "\n",
        "    X_train_final, y_train_final = create_windows(train_data_final, best_params['lookback'])\n",
        "    X_test_final, y_test_final = create_windows(test_data_final, best_params['lookback'])\n",
        "\n",
        "    final_input_shape = (best_params['lookback'], X_train_final.shape[2])\n",
        "    if model_name == 'Elman':\n",
        "        final_model = model_builder(input_shape=final_input_shape, hidden_units=best_params['hidden_units'], dropout=best_params['dropout'])\n",
        "    elif model_name == 'Multi-Layer':\n",
        "        final_model = model_builder(input_shape=final_input_shape, hidden_units=best_params['hidden_units'], dropout=best_params['dropout'])\n",
        "    elif model_name == 'Jordan':\n",
        "        final_model = model_builder(hidden_units=best_params['hidden_units'])\n",
        "        final_model.build(final_input_shape)\n",
        "\n",
        "    final_model.compile(optimizer=keras.optimizers.Adam(learning_rate=best_params['lr']), loss='mse')\n",
        "\n",
        "    final_model.fit(X_train_final, y_train_final, epochs=N_EPOCHS, batch_size=BATCH_SIZE, verbose=0)\n",
        "\n",
        "    final_preds_scaled = final_model.predict(X_test_final, verbose=0)\n",
        "    final_preds_unscaled = scaler_y_final.inverse_transform(final_preds_scaled)\n",
        "    final_true_unscaled = scaler_y_final.inverse_transform(y_test_final.reshape(-1, 1))\n",
        "\n",
        "    final_test_rmse = math.sqrt(mean_squared_error(final_true_unscaled, final_preds_unscaled))\n",
        "    print(f\"  -> Final Test Set RMSE for {model_name}: {final_test_rmse:.2f}\")\n",
        "\n",
        "    final_results[model_name] = {\n",
        "        'best_params': best_params,\n",
        "        'test_rmse': final_test_rmse,\n",
        "        'predictions': final_preds_unscaled.flatten(),\n",
        "        'true_values': final_true_unscaled.flatten(),\n",
        "        'test_dates': test_df.index[best_params['lookback']:]\n",
        "    }\n",
        "\n",
        "print(f\"\\n{'='*40}\\nFINAL RESULTS SUMMARY\\n{'='*40}\")\n",
        "best_overall_model_name = None\n",
        "lowest_rmse = float('inf')\n",
        "\n",
        "for model_name, result in final_results.items():\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"  - Best Hyperparameters: {result['best_params']}\")\n",
        "    print(f\"  - Final Test RMSE: {result['test_rmse']:.2f}\")\n",
        "    if result['test_rmse'] < lowest_rmse:\n",
        "        lowest_rmse = result['test_rmse']\n",
        "        best_overall_model_name = model_name\n",
        "\n",
        "print(f\"\\n Overall Best Performing Model: {best_overall_model_name} (RMSE: {lowest_rmse:.2f})\")\n"
      ],
      "metadata": {
        "id": "wQeGeK5oQojd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = kagglehub.dataset_download(\"ujjwalchowdhury/walmartcleaned\")\n",
        "print(os.listdir(path))\n",
        "file_name = \"walmart_cleaned.csv\"\n",
        "file_path = os.path.join(path, file_name)\n",
        "df = pd.read_csv(file_path)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "eW3aQyiQxYa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "JWogsxvszF1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FILE_PATH = os.path.join(path, file_name)\n",
        "\n",
        "STORE_ID = 1\n",
        "DEPT_ID = 1\n",
        "\n",
        "TARGET_COLUMN = \"weekly_sales\"\n",
        "NUMERIC_FEATURES = [\"temperature\", \"fuel_price\", \"markdown1\", \"markdown2\", \"markdown3\", \"markdown4\", \"markdown5\", \"cpi\", \"unemployment\", \"size\"]\n",
        "CATEGORICAL_FEATURES = [\"isholiday\", \"type\"]\n",
        "\n",
        "\n",
        "HYPERPARAMETERS_TO_TEST = {\n",
        "    'lookback': [52],\n",
        "    'hidden_units': [32],\n",
        "    'learning_rate': [0.001],\n",
        "    'dropout': [0.2],\n",
        "}\n",
        "N_EPOCHS = 20\n",
        "PATIENCE = 8\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "TEST_SET_FRACTION = 0.20\n",
        "N_CV_SPLITS = 3\n",
        "VALIDATION_BLOCK_SIZE = 20\n",
        "\n",
        "\n",
        "def load_and_prepare_data(filepath):\n",
        "    df = pd.read_csv(filepath)\n",
        "    df.columns = df.columns.str.strip().str.lower()\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    return df.set_index('date').sort_index()\n",
        "\n",
        "def add_time_features(df):\n",
        "    df_copy = df.copy()\n",
        "    df_copy['week'] = df_copy.index.isocalendar().week.astype(int)\n",
        "    df_copy['week_sin'] = np.sin(2 * np.pi * (df_copy['week'] - 1) / 52)\n",
        "    df_copy['week_cos'] = np.cos(2 * np.pi * (df_copy['week'] - 1) / 52)\n",
        "    df_copy['time_idx'] = np.arange(len(df_copy))\n",
        "    return df_copy\n",
        "\n",
        "def create_windows(data, lookback):\n",
        "    X, y = [], []\n",
        "    target_column_index = 0\n",
        "    for i in range(len(data) - lookback):\n",
        "        X.append(data[i:(i + lookback)])\n",
        "        y.append(data[i + lookback, target_column_index])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def get_cv_splits(data_length, n_splits, val_size):\n",
        "    initial_train_size = data_length - (n_splits * val_size)\n",
        "    for i in range(n_splits):\n",
        "        train_end = initial_train_size + i * val_size\n",
        "        val_end = train_end + val_size\n",
        "        yield range(0, train_end), range(train_end, val_end)\n",
        "\n",
        "def build_elman_model(input_shape, hidden_units, dropout):\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=input_shape),\n",
        "        layers.SimpleRNN(hidden_units, activation='tanh', dropout=dropout),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_multi_layer_model(input_shape, hidden_units, dropout):\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=input_shape),\n",
        "        layers.SimpleRNN(hidden_units, activation='tanh', return_sequences=True, dropout=dropout),\n",
        "        layers.SimpleRNN(hidden_units, activation='tanh', dropout=dropout),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "class JordanRNN(keras.Model):\n",
        "    def __init__(self, hidden_units):\n",
        "        super().__init__()\n",
        "        self.rnn_cell = layers.SimpleRNNCell(hidden_units, activation='tanh')\n",
        "        # The final output layer.\n",
        "        self.dense_output = layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        hidden_state = tf.zeros((batch_size, self.rnn_cell.units))\n",
        "        previous_output = tf.zeros((batch_size, 1))\n",
        "\n",
        "        for t in range(inputs.shape[1]):\n",
        "            current_features = inputs[:, t, :]\n",
        "            # Combine current features with the previous output.\n",
        "            combined_input = tf.concat([current_features, previous_output], axis=1)\n",
        "            _, [hidden_state] = self.rnn_cell(combined_input, [hidden_state])\n",
        "\n",
        "            # Calculate the output used in the next iteration.\n",
        "            previous_output = self.dense_output(hidden_state)\n",
        "\n",
        "        return previous_output\n",
        "\n",
        "\n",
        "print(\"--- Starting Walmart Sales Prediction Pipeline ---\")\n",
        "\n",
        "full_df = load_and_prepare_data(FILE_PATH)\n",
        "series_df = full_df[(full_df['store'] == STORE_ID) & (full_df['dept'] == DEPT_ID)].copy()\n",
        "\n",
        "series_df = series_df.ffill().bfill()\n",
        "\n",
        "test_size = int(len(series_df) * TEST_SET_FRACTION)\n",
        "trainval_df = series_df.iloc[:-test_size].copy()\n",
        "\n",
        "test_df = series_df.iloc[-test_size:].copy()\n",
        "\n",
        "models_to_evaluate = {\n",
        "    'Elman': build_elman_model,\n",
        "    'Multi-Layer': build_multi_layer_model,\n",
        "    'Jordan': JordanRNN\n",
        "}\n",
        "\n",
        "param_grid = HYPERPARAMETERS_TO_TEST\n",
        "param_combos = list(itertools.product(\n",
        "    param_grid['lookback'], param_grid['hidden_units'],\n",
        "    param_grid['learning_rate'], param_grid['dropout']\n",
        "))\n",
        "\n",
        "final_results = {}\n",
        "\n",
        "for model_name, model_builder in models_to_evaluate.items():\n",
        "    print(f\"\\n{'='*40}\\nStarting evaluation for: {model_name}\\n{'='*40}\")\n",
        "\n",
        "    best_params = {}\n",
        "    best_avg_rmse = float('inf')\n",
        "\n",
        "    for lookback, hidden_units, lr, dropout in param_combos:\n",
        "        print(f\"  Testing params: lookback={lookback}, hidden={hidden_units}, lr={lr}, dropout={dropout}\")\n",
        "        fold_rmses = []\n",
        "        cv_splits = get_cv_splits(len(trainval_df), N_CV_SPLITS, VALIDATION_BLOCK_SIZE)\n",
        "\n",
        "        for train_idx, val_idx in cv_splits:\n",
        "            train_fold = trainval_df.iloc[train_idx].copy()\n",
        "            val_fold = trainval_df.iloc[val_idx].copy()\n",
        "\n",
        "            train_fold = add_time_features(train_fold)\n",
        "            val_fold = add_time_features(val_fold)\n",
        "\n",
        "            train_dummies = pd.get_dummies(train_fold['type'], prefix='type')\n",
        "            val_dummies = pd.get_dummies(val_fold['type'], prefix='type').reindex(columns=train_dummies.columns, fill_value=0)\n",
        "            train_fold = pd.concat([train_fold, train_dummies], axis=1)\n",
        "            val_fold = pd.concat([val_fold, val_dummies], axis=1)\n",
        "\n",
        "            time_features = ['week_sin', 'week_cos', 'time_idx']\n",
        "            all_features = NUMERIC_FEATURES + time_features + ['isholiday'] + list(train_dummies.columns)\n",
        "            all_features = [f for f in all_features if f in train_fold.columns]\n",
        "\n",
        "            scaler_X = StandardScaler()\n",
        "            scaler_y = StandardScaler()\n",
        "\n",
        "            train_fold[all_features] = scaler_X.fit_transform(train_fold[all_features])\n",
        "            train_fold[[TARGET_COLUMN]] = scaler_y.fit_transform(train_fold[[TARGET_COLUMN]])\n",
        "            val_fold[all_features] = scaler_X.transform(val_fold[all_features])\n",
        "            val_fold[[TARGET_COLUMN]] = scaler_y.transform(val_fold[[TARGET_COLUMN]])\n",
        "\n",
        "            train_data_for_windows = train_fold[[TARGET_COLUMN] + all_features].values\n",
        "            val_context = pd.concat([train_fold.tail(lookback), val_fold])\n",
        "            val_data_for_windows = val_context[[TARGET_COLUMN] + all_features].values\n",
        "\n",
        "            X_train, y_train = create_windows(train_data_for_windows, lookback)\n",
        "            X_val, y_val = create_windows(val_data_for_windows, lookback)\n",
        "\n",
        "            if X_train.shape[0] == 0 or X_val.shape[0] == 0: continue\n",
        "\n",
        "            input_shape = (lookback, X_train.shape[2])\n",
        "\n",
        "\n",
        "            if model_name == 'Elman':\n",
        "                model = model_builder(input_shape=input_shape, hidden_units=hidden_units, dropout=dropout)\n",
        "            elif model_name == 'Multi-Layer':\n",
        "                model = model_builder(input_shape=input_shape, hidden_units=hidden_units, dropout=dropout)\n",
        "            elif model_name == 'Jordan':\n",
        "                model = model_builder(hidden_units=hidden_units)\n",
        "                model.build(input_shape)\n",
        "\n",
        "            model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), loss='mse')\n",
        "            early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
        "\n",
        "            model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
        "                      epochs=N_EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping], verbose=0)\n",
        "\n",
        "            preds_scaled = model.predict(X_val, verbose=0)\n",
        "            preds_unscaled = scaler_y.inverse_transform(preds_scaled)\n",
        "            true_unscaled = scaler_y.inverse_transform(y_val.reshape(-1, 1))\n",
        "            fold_rmse = math.sqrt(mean_squared_error(true_unscaled, preds_unscaled))\n",
        "            fold_rmses.append(fold_rmse)\n",
        "\n",
        "        avg_rmse = np.mean(fold_rmses) if fold_rmses else float('inf')\n",
        "        if avg_rmse < best_avg_rmse:\n",
        "            best_avg_rmse = avg_rmse\n",
        "            best_params = {'lookback': lookback, 'hidden_units': hidden_units, 'lr': lr, 'dropout': dropout}\n",
        "\n",
        "    print(f\"\\n  Best parameters for {model_name}: {best_params} (Avg. CV RMSE: ${best_avg_rmse:,.2f})\")\n",
        "\n",
        "\n",
        "    train_final = add_time_features(trainval_df)\n",
        "    test_final = add_time_features(test_df)\n",
        "\n",
        "    train_dummies_final = pd.get_dummies(train_final['type'], prefix='type')\n",
        "    test_dummies_final = pd.get_dummies(test_final['type'], prefix='type').reindex(columns=train_dummies_final.columns, fill_value=0)\n",
        "    train_final = pd.concat([train_final, train_dummies_final], axis=1)\n",
        "    test_final = pd.concat([test_final, test_dummies_final], axis=1)\n",
        "\n",
        "    all_features_final = NUMERIC_FEATURES + time_features + ['isholiday'] + list(train_dummies_final.columns)\n",
        "    all_features_final = [f for f in all_features_final if f in train_final.columns]\n",
        "\n",
        "    scaler_X_final = StandardScaler()\n",
        "    scaler_y_final = StandardScaler()\n",
        "\n",
        "    train_final[all_features_final] = scaler_X_final.fit_transform(train_final[all_features_final])\n",
        "    train_final[[TARGET_COLUMN]] = scaler_y_final.fit_transform(train_final[[TARGET_COLUMN]])\n",
        "    test_final[all_features_final] = scaler_X_final.transform(test_final[all_features_final])\n",
        "    test_final[[TARGET_COLUMN]] = scaler_y_final.transform(test_final[[TARGET_COLUMN]])\n",
        "\n",
        "    train_data_final = train_final[[TARGET_COLUMN] + all_features_final].values\n",
        "    test_context_final = pd.concat([train_final.tail(best_params['lookback']), test_final])\n",
        "    test_data_final = test_context_final[[TARGET_COLUMN] + all_features_final].values\n",
        "\n",
        "    X_train_final, y_train_final = create_windows(train_data_final, best_params['lookback'])\n",
        "    X_test_final, y_test_final = create_windows(test_data_final, best_params['lookback'])\n",
        "\n",
        "    final_input_shape = (best_params['lookback'], X_train_final.shape[2])\n",
        "    if model_name == 'Elman':\n",
        "        final_model = model_builder(input_shape=final_input_shape, hidden_units=best_params['hidden_units'], dropout=best_params['dropout'])\n",
        "    elif model_name == 'Multi-Layer':\n",
        "        final_model = model_builder(input_shape=final_input_shape, hidden_units=best_params['hidden_units'], dropout=best_params['dropout'])\n",
        "    elif model_name == 'Jordan':\n",
        "        final_model = model_builder(hidden_units=best_params['hidden_units'])\n",
        "        final_model.build(final_input_shape) # Build explicitly\n",
        "    final_model.compile(optimizer=keras.optimizers.Adam(learning_rate=best_params['lr']), loss='mse')\n",
        "\n",
        "    final_model.fit(X_train_final, y_train_final, epochs=N_EPOCHS, batch_size=BATCH_SIZE, verbose=0)\n",
        "\n",
        "    final_preds_scaled = final_model.predict(X_test_final, verbose=0)\n",
        "    final_preds_unscaled = scaler_y_final.inverse_transform(final_preds_scaled)\n",
        "    final_true_unscaled = scaler_y_final.inverse_transform(y_test_final.reshape(-1, 1))\n",
        "\n",
        "    final_test_rmse = math.sqrt(mean_squared_error(final_true_unscaled, final_preds_unscaled))\n",
        "    print(f\"Final Test Set RMSE for {model_name}: ${final_test_rmse:,.2f}\")\n",
        "\n",
        "    final_results[model_name] = {\n",
        "        'best_params': best_params,\n",
        "        'test_rmse': final_test_rmse,\n",
        "        'predictions': final_preds_unscaled.flatten(),\n",
        "        'true_values': final_true_unscaled.flatten(),\n",
        "        'test_dates': test_df.index[best_params['lookback']:]\n",
        "    }\n",
        "\n",
        "\n",
        "print(f\"\\n{'='*40}\\nFINAL RESULTS SUMMARY (Store {STORE_ID}, Dept {DEPT_ID})\\n{'='*40}\")\n",
        "best_overall_model_name = None\n",
        "lowest_rmse = float('inf')\n",
        "\n",
        "for model_name, result in final_results.items():\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"  - Best Hyperparameters: {result['best_params']}\")\n",
        "    print(f\"  - Final Test RMSE: ${result['test_rmse']:,.2f}\")\n",
        "    if result['test_rmse'] < lowest_rmse:\n",
        "        lowest_rmse = result['test_rmse']\n",
        "        best_overall_model_name = model_name\n",
        "\n",
        "print(f\"\\n Overall Best Performing Model: {best_overall_model_name} (RMSE: ${lowest_rmse:,.2f})\")"
      ],
      "metadata": {
        "id": "aYu1PHs8YKgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = kagglehub.dataset_download(\"die9origephit/human-activity-recognition\")\n",
        "file_name = \"time_series_data_human_activities.csv\"\n",
        "file_path = os.path.join(path, file_name)\n",
        "df = pd.read_csv(file_path)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "MvdT-kNR1rur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, math, itertools, copy, pickle, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from statsmodels.tsa.stattools import adfuller, kpss\n",
        "\n",
        "\n",
        "DATASET_DIR = globals().get(\"path\", \".\")\n",
        "FILE_NAME = \"time_series_data_human_activities.csv\"\n",
        "FILE_PATH = os.path.join(DATASET_DIR, FILE_NAME)\n",
        "\n",
        "RESULTS_DIR = \"har_rnn_results\"\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "USER_COL = \"user\"\n",
        "ACTIVITY_COL = \"activity\"\n",
        "TIMESTAMP_COL = \"timestamp\"\n",
        "SENSOR_COLS = [\"x-axis\", \"y-axis\", \"z-axis\"]\n",
        "\n",
        "LOOKBACK_CANDIDATES = [64]\n",
        "HORIZON = 0\n",
        "STRIDE = 8\n",
        "MAX_SAMPLES = None\n",
        "BATCH_SIZE = 128\n",
        "N_EPOCHS = 10\n",
        "PATIENCE = 6\n",
        "N_CV_SPLITS = 3\n",
        "VAL_BLOCK_SIZE = 100000\n",
        "GAP = 0\n",
        "VERBOSE = True\n",
        "\n",
        "HYPERPARAM_GRID = {\n",
        "    'lookback': LOOKBACK_CANDIDATES,\n",
        "    'hidden': [64],\n",
        "    'lr': [1e-3],\n",
        "    'dropout': [0.2]\n",
        "}\n",
        "\n",
        "print(\"FILE_PATH:\", FILE_PATH)\n",
        "\n",
        "def try_parse_timestamp(s):\n",
        "    ts = pd.to_numeric(s, errors='coerce')\n",
        "    candidates = ['ns', 'us', 'ms', 's']\n",
        "    for unit in candidates:\n",
        "        try:\n",
        "            dt = pd.to_datetime(ts, unit=unit, errors='coerce')\n",
        "            median_year = pd.Series(dt).dt.year.dropna().median()\n",
        "            if not np.isnan(median_year) and 1980 <= median_year <= 2035:\n",
        "                return dt, unit\n",
        "        except Exception:\n",
        "            pass\n",
        "    try:\n",
        "        dt = pd.to_datetime(s, errors='coerce')\n",
        "        median_year = pd.Series(dt).dt.year.dropna().median()\n",
        "        if not np.isnan(median_year) and 1980 <= median_year <= 2035:\n",
        "            return dt, 'parsed'\n",
        "    except Exception:\n",
        "        pass\n",
        "    return None, None\n",
        "\n",
        "def read_har_csv(filepath, max_rows=None):\n",
        "    df = pd.read_csv(filepath)\n",
        "    df.columns = df.columns.str.strip().str.lower()\n",
        "    for c in [USER_COL.lower(), ACTIVITY_COL.lower(), TIMESTAMP_COL.lower()]:\n",
        "        if c not in df.columns:\n",
        "            raise ValueError(f\"Expected column '{c}' in CSV. Found: {df.columns.tolist()}\")\n",
        "    df = df.rename(columns={USER_COL.lower(): 'user', ACTIVITY_COL.lower(): 'activity', TIMESTAMP_COL.lower(): 'timestamp'})\n",
        "    dt, unit = try_parse_timestamp(df['timestamp'])\n",
        "    df['timestamp'] = dt\n",
        "    df = df.dropna(subset=['timestamp']).copy()\n",
        "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
        "    if max_rows is not None:\n",
        "        df = df.iloc[:max_rows].copy()\n",
        "    return df\n",
        "\n",
        "\n",
        "def stationarity_report_numeric(series, name='series'):\n",
        "    try:\n",
        "        a = adfuller(series.dropna(), autolag='AIC')\n",
        "        kst = kpss(series.dropna(), regression='c', nlags=\"auto\")\n",
        "        print(f\"ADF p={a[1]:.4f}; KPSS p={kst[1]:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(\"Stationarity test failed:\", e)\n",
        "\n",
        "def window_label_mode(labels):\n",
        "    c = Counter(labels)\n",
        "    most = c.most_common()\n",
        "    return most[0][0]\n",
        "\n",
        "def make_windows_classification(df, feature_cols, label_col, lookback, stride=1):\n",
        "    arr_X = df[feature_cols].values\n",
        "    arr_y = df[label_col].values\n",
        "    T = len(df)\n",
        "    starts = list(range(0, T - lookback + 1, stride))\n",
        "    Xs = []\n",
        "    Ys = []\n",
        "    for s in starts:\n",
        "        window_X = arr_X[s: s+lookback]\n",
        "        window_y = arr_y[s: s+lookback]\n",
        "        Xs.append(window_X)\n",
        "        Ys.append(window_label_mode(window_y))\n",
        "    if len(Xs)==0:\n",
        "        return np.empty((0, lookback, len(feature_cols))), np.empty((0,), dtype=np.int32)\n",
        "    return np.array(Xs, dtype=np.float32), np.array(Ys)\n",
        "\n",
        "def rolling_origin_splits_by_index(n_time, n_splits=N_CV_SPLITS, val_block_size=VAL_BLOCK_SIZE, gap=GAP, initial_train_size=None):\n",
        "    if initial_train_size is None:\n",
        "        initial_train_size = n_time - n_splits * val_block_size - gap * n_splits\n",
        "    for i in range(n_splits):\n",
        "        train_end = initial_train_size + i * val_block_size\n",
        "        val_start = train_end + gap\n",
        "        val_end = val_start + val_block_size\n",
        "        if val_end > n_time:\n",
        "            break\n",
        "        train_idx = np.arange(0, train_end)\n",
        "        val_idx = np.arange(val_start, val_end)\n",
        "        yield train_idx, val_idx\n",
        "\n",
        "def build_elman_classifier(input_shape, n_classes, hidden_size=64, dropout=0.0):\n",
        "    inp = keras.Input(shape=input_shape)\n",
        "    x = layers.SimpleRNN(hidden_size, activation='tanh', dropout=dropout, return_sequences=False)(inp)\n",
        "    out = layers.Dense(n_classes, activation='softmax')(x)\n",
        "    return keras.Model(inp, out)\n",
        "\n",
        "def build_multi_classifier(input_shape, n_classes, hidden_size=64, dropout=0.2):\n",
        "    inp = keras.Input(shape=input_shape)\n",
        "    x = layers.SimpleRNN(hidden_size, activation='tanh', dropout=dropout, return_sequences=True)(inp)\n",
        "    x = layers.SimpleRNN(hidden_size, activation='tanh', dropout=dropout, return_sequences=False)(x)\n",
        "    out = layers.Dense(n_classes, activation='softmax')(x)\n",
        "    return keras.Model(inp, out)\n",
        "\n",
        "# this is the big mans Jordan implementation. If you readijng this I couldnt get mione to work to be honest. My bad :)\n",
        "class JordanClassifier(keras.Model):\n",
        "    def __init__(self, input_size, n_classes, hidden_size=64, dropout=0.0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.rnn_cell = layers.SimpleRNNCell(hidden_size, activation='tanh')\n",
        "        self.dropout = layers.Dropout(dropout)\n",
        "        self.fc = layers.Dense(n_classes, activation='softmax')\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        batch = tf.shape(x)[0]\n",
        "        h = tf.zeros((batch, self.hidden_size))\n",
        "        y_prev = tf.zeros((batch, 1))  # previous *class* vector isn't available, we use previous logits as scalar 0 -> better: use zeros\n",
        "        # We'll feed previous output scalar (0) concatenated — this is a simplistic Jordan adaptation for classification\n",
        "        for t in range(x.shape[1]):\n",
        "            xt = x[:, t, :]\n",
        "            inp_t = tf.concat([xt, y_prev], axis=1)\n",
        "            out_cell, [h] = self.rnn_cell(inp_t, [h])\n",
        "            if training:\n",
        "                h = self.dropout(h, training=training)\n",
        "            # produce logits then reduce to a scalar to feed next step: take mean(logits) as scalar\n",
        "            logits = self.fc(h)  # (batch, n_classes)\n",
        "            # reduce to scalar in [-1,1] via tanh of mean\n",
        "            y_prev = tf.expand_dims(tf.tanh(tf.reduce_mean(logits, axis=1)), axis=1)\n",
        "        # final logits computed above (logits) -> but we need to return final class probs\n",
        "        return logits\n",
        "\n",
        "def compile_and_train(model, X_train, y_train, X_val, y_val, cfg):\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=cfg['lr']),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    es = keras.callbacks.EarlyStopping(patience=cfg['patience'], restore_best_weights=True, verbose=0)\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
        "                        epochs=cfg['epochs'], batch_size=cfg['batch_size'],\n",
        "                        callbacks=[es], verbose=cfg.get('verbose', 0))\n",
        "    return model, history\n",
        "\n",
        "def evaluate_classification(model, X, y, label_encoder, n_classes):\n",
        "    preds_prob = model.predict(X, batch_size=256)\n",
        "    preds = np.argmax(preds_prob, axis=1)\n",
        "    acc = accuracy_score(y, preds)\n",
        "    report = classification_report(y, preds, target_names=label_encoder.classes_, labels=np.arange(n_classes), zero_division=0)\n",
        "    return acc, report, preds\n",
        "\n",
        "\n",
        "def pipeline_har_classification(filepath):\n",
        "    df = read_har_csv(filepath, max_rows=MAX_SAMPLES)\n",
        "    print(\"Loaded rows:\", len(df))\n",
        "    sensor_cols = [c for c in df.columns if c in [s.lower() for s in SENSOR_COLS]]\n",
        "    if len(sensor_cols) == 0:\n",
        "        sensor_cols = SENSOR_COLS\n",
        "    le = LabelEncoder()\n",
        "    df['activity_enc'] = le.fit_transform(df['activity'].astype(str))\n",
        "    n_classes = len(le.classes_)\n",
        "    users = sorted(df['user'].unique())\n",
        "    user_map = {u: i for i, u in enumerate(users)}\n",
        "    df['user_enc'] = df['user'].map(user_map)\n",
        "    user_one_hot = False\n",
        "    if len(users) <= 128:\n",
        "        user_one_hot = True\n",
        "        user_dummies = pd.get_dummies(df['user_enc'], prefix='usr')\n",
        "        df = pd.concat([df.reset_index(drop=True), user_dummies.reset_index(drop=True)], axis=1)\n",
        "        user_cols = [c for c in df.columns if c.startswith('usr_')]\n",
        "    feature_cols = sensor_cols + user_cols\n",
        "\n",
        "    df = df.set_index('timestamp').sort_index()\n",
        "\n",
        "    df[feature_cols] = df[feature_cols].interpolate(method='time').ffill().bfill()\n",
        "    n = len(df)\n",
        "    test_n = max(1, int(0.10 * n))\n",
        "    trainval_df = df.iloc[:-test_n].copy()\n",
        "    test_df = df.iloc[-test_n:].copy()\n",
        "\n",
        "    n_time = len(trainval_df)\n",
        "    combos = list(itertools.product(HYPERPARAM_GRID['lookback'], HYPERPARAM_GRID['hidden'],\n",
        "                                    HYPERPARAM_GRID['lr'], HYPERPARAM_GRID['dropout']))\n",
        "    results = {}\n",
        "\n",
        "    for model_type in ['Elman', 'Jordan', 'Multi']:\n",
        "        print(\"\\n=== MODEL:\", model_type, \"===\")\n",
        "        combo_scores = []\n",
        "        for (lb, hid, lr, drop) in combos:\n",
        "            fold_accs = []\n",
        "            for (train_idx, val_idx) in rolling_origin_splits_by_index(n_time, n_splits=N_CV_SPLITS, val_block_size=VAL_BLOCK_SIZE, gap=GAP):\n",
        "                train_fold = trainval_df.iloc[train_idx].copy()\n",
        "                val_fold = trainval_df.iloc[val_idx].copy()\n",
        "                scaler = StandardScaler()\n",
        "                scaler.fit(train_fold[sensor_cols])\n",
        "                train_fold[sensor_cols] = scaler.transform(train_fold[sensor_cols])\n",
        "                val_fold[sensor_cols] = scaler.transform(val_fold[sensor_cols])\n",
        "                X_train, y_train = make_windows_classification(train_fold, feature_cols, 'activity_enc', lookback=lb, stride=STRIDE)\n",
        "                val_context = pd.concat([train_fold.tail(lb), val_fold])\n",
        "                X_val, y_val = make_windows_classification(val_context, feature_cols, 'activity_enc', lookback=lb, stride=STRIDE)\n",
        "                if X_train.shape[0]==0 or X_val.shape[0]==0:\n",
        "                    fold_accs.append(np.nan)\n",
        "                    continue\n",
        "\n",
        "                input_shape = (lb, X_train.shape[2])\n",
        "                if model_type == 'Elman':\n",
        "                    model = build_elman_classifier(input_shape, n_classes, hidden_size=hid, dropout=drop)\n",
        "                elif model_type == 'Multi':\n",
        "                    model = build_multi_classifier(input_shape, n_classes, hidden_size=hid, dropout=drop)\n",
        "                elif model_type == 'Jordan':\n",
        "                    model = JordanClassifier(input_size=X_train.shape[2], n_classes=n_classes, hidden_size=hid, dropout=drop)\n",
        "                    model.build((None, lb, X_train.shape[2]))\n",
        "                else:\n",
        "                    raise ValueError(\"Unknown model type\")\n",
        "\n",
        "                cfg = {'epochs': N_EPOCHS, 'lr': lr, 'batch_size': BATCH_SIZE, 'patience': PATIENCE, 'verbose': 0}\n",
        "                model, history = compile_and_train(model, X_train, y_train, X_val, y_val, cfg)\n",
        "                acc, rep, preds = evaluate_classification(model, X_val, y_val, le, n_classes)\n",
        "                fold_accs.append(acc)\n",
        "            fold_accs = [v for v in fold_accs if not np.isnan(v)]\n",
        "            mean_acc = np.mean(fold_accs) if len(fold_accs)>0 else np.nan\n",
        "            combo_scores.append({'lookback':lb, 'hidden':hid, 'lr':lr, 'dropout':drop, 'mean_cv_acc': mean_acc})\n",
        "\n",
        "        combo_scores_valid = [c for c in combo_scores if not np.isnan(c['mean_cv_acc'])]\n",
        "\n",
        "        best_combo = sorted(combo_scores_valid, key=lambda x: -x['mean_cv_acc'])[0]\n",
        "        print(\"Best combo:\", best_combo)\n",
        "        results[model_type] = {'best_combo': best_combo, 'combo_scores': combo_scores}\n",
        "\n",
        "        train_full = trainval_df.copy()\n",
        "        test_full = test_df.copy()\n",
        "        scaler = StandardScaler(); scaler.fit(train_full[sensor_cols])\n",
        "        train_full[sensor_cols] = scaler.transform(train_full[sensor_cols])\n",
        "        test_full[sensor_cols] = scaler.transform(test_full[sensor_cols])\n",
        "        lb = best_combo['lookback']; hid = best_combo['hidden']; lr = best_combo['lr']; drop = best_combo['dropout']\n",
        "        X_train_all, y_train_all = make_windows_classification(train_full, feature_cols, 'activity_enc', lookback=lb, stride=STRIDE)\n",
        "        test_context = pd.concat([train_full.tail(lb), test_full])\n",
        "        X_test, y_test = make_windows_classification(test_context, feature_cols, 'activity_enc', lookback=lb, stride=STRIDE)\n",
        "\n",
        "        if model_type == 'Elman':\n",
        "            final_model = build_elman_classifier((lb, X_train_all.shape[2]), n_classes, hidden_size=hid, dropout=drop)\n",
        "        elif model_type == 'Multi':\n",
        "            final_model = build_multi_classifier((lb, X_train_all.shape[2]), n_classes, hidden_size=hid, dropout=drop)\n",
        "        else:\n",
        "            final_model = JordanClassifier(input_size=X_train_all.shape[2], n_classes=n_classes, hidden_size=hid, dropout=drop)\n",
        "            final_model.build((None, lb, X_train_all.shape[2]))\n",
        "\n",
        "        cfg = {'epochs': N_EPOCHS, 'lr': lr, 'batch_size': BATCH_SIZE, 'patience': PATIENCE, 'verbose': 1}\n",
        "        final_model, history = compile_and_train(final_model, X_train_all, y_train_all, X_train_all[:0], y_train_all[:0], cfg)\n",
        "        acc_test, report_test, preds_test = evaluate_classification(final_model, X_test, y_test, le, n_classes)\n",
        "        print(f\" Final TEST accuracy for {model_type}: {acc_test:.4f}\")\n",
        "        print(\"Classification report (test):\")\n",
        "        print(report_test)\n",
        "\n",
        "        results[model_type].update({\n",
        "            'final_model': final_model,\n",
        "            'scaler': scaler,\n",
        "            'test_acc': acc_test,\n",
        "            'test_report': report_test,\n",
        "            'y_test': y_test,\n",
        "            'y_pred': preds_test\n",
        "        })\n",
        "\n",
        "\n",
        "    # summary\n",
        "    summary = {m: (results[m].get('test_acc', None)) for m in results}\n",
        "    print(\"Summary test accuracies:\", summary)\n",
        "\n",
        "start = time.time()\n",
        "res = pipeline_har_classification(FILE_PATH)"
      ],
      "metadata": {
        "id": "M727GrWc2i6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = kagglehub.dataset_download(\"krupalpatel07/microsoft-stock-data\")\n",
        "file_name = \"MSFT.csv\"\n",
        "file_path = os.path.join(path, file_name)\n",
        "df = pd.read_csv(file_path)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "N6XOAGiC7Nd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, math, itertools, copy, pickle, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(style='whitegrid')\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from statsmodels.tsa.stattools import adfuller, kpss\n",
        "\n",
        "\n",
        "DATASET_DIR = globals().get('path', '.')\n",
        "FILE_NAME = \"MSFT.csv\"\n",
        "FILE_PATH = os.path.join(DATASET_DIR, FILE_NAME)\n",
        "\n",
        "RESULTS_DIR = \"msft_rnn_results\"\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "TIME_COL = \"date\"\n",
        "TARGET_COL = \"close\"\n",
        "FEATURE_COLS_BASE = [\"open\",\"high\",\"low\",\"volume\"]\n",
        "\n",
        "LOOKBACK_CANDIDATES = [30]\n",
        "HORIZON = 1\n",
        "BATCH_SIZE = 128\n",
        "N_EPOCHS = 10\n",
        "PATIENCE = 3\n",
        "N_CV_SPLITS = 3\n",
        "VAL_BLOCK_SIZE = 252\n",
        "GAP = 0\n",
        "TEST_FRAC = 0.10\n",
        "VERBOSE = True\n",
        "\n",
        "HYPERPARAM_GRID = {\n",
        "    'lookback': LOOKBACK_CANDIDATES,\n",
        "    'hidden': [32],\n",
        "    'lr': [1e-3],\n",
        "    'dropout': [0.2]\n",
        "}\n",
        "\n",
        "\n",
        "def read_msft_csv(filepath):\n",
        "    df = pd.read_csv(filepath)\n",
        "    df.columns = df.columns.str.strip().str.lower()\n",
        "    if TIME_COL not in df.columns:\n",
        "        for alt in ['date','day','trade_date']:\n",
        "            if alt in df.columns:\n",
        "                df = df.rename(columns={alt: TIME_COL})\n",
        "                break\n",
        "    df[TIME_COL] = pd.to_datetime(df[TIME_COL], errors='coerce')\n",
        "    if df[TIME_COL].isna().any():\n",
        "        df = df.dropna(subset=[TIME_COL]).copy()\n",
        "    df = df.sort_values(TIME_COL).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "import pandas as pd\n",
        "from statsmodels.tsa.stattools import adfuller, kpss\n",
        "\n",
        "def adf_test(series):\n",
        "    res = adfuller(series.dropna(), autolag='AIC')\n",
        "    return {'stat': res[0], 'pval': res[1], 'usedlag': res[2], 'nobs': res[3], 'crit': res[4]}\n",
        "\n",
        "def kpss_test(series, regression='c'):\n",
        "    stat, pval, nlags, crit = kpss(series.dropna(), regression=regression, nlags=\"auto\")\n",
        "    return {'stat': stat, 'pval': pval, 'nlags': nlags, 'crit': crit}\n",
        "\n",
        "def stationarity_report_df(df, exclude_cols=None):\n",
        "    if exclude_cols is None:\n",
        "        exclude_cols = []\n",
        "    try:\n",
        "      results = []\n",
        "      for col in df.select_dtypes(include='number').columns:\n",
        "          if col in exclude_cols:\n",
        "              continue\n",
        "          series = df[col].dropna()\n",
        "\n",
        "          try:\n",
        "              a = adf_test(series)\n",
        "              k = kpss_test(series)\n",
        "          except Exception as e:\n",
        "              print(f\"Skipping {col}: {e}\")\n",
        "              continue\n",
        "\n",
        "          adf_stationary = a['pval'] < 0.05\n",
        "          kpss_stationary = k['pval'] > 0.05\n",
        "\n",
        "          if adf_stationary and kpss_stationary:\n",
        "              conclusion = \"Stationary\"\n",
        "          elif not adf_stationary and not kpss_stationary:\n",
        "              conclusion = \"Non-stationary\"\n",
        "          else:\n",
        "              conclusion = \"Mixed/Borderline\"\n",
        "\n",
        "          results.append({\n",
        "              \"Column\": col,\n",
        "              \"ADF stat\": a['stat'],\n",
        "              \"ADF p\": a['pval'],\n",
        "              \"ADF result\": \"Stationary\" if adf_stationary else \"Non-stationary\",\n",
        "              \"KPSS stat\": k['stat'],\n",
        "              \"KPSS p\": k['pval'],\n",
        "              \"KPSS result\": \"Stationary\" if kpss_stationary else \"Non-stationary\",\n",
        "              \"Overall conclusion\": conclusion\n",
        "          })\n",
        "      summary = pd.DataFrame(results)\n",
        "      return summary\n",
        "    except Exception as e:\n",
        "      print(f\"Error: {e}\")\n",
        "      return None\n",
        "\n",
        "def add_time_features_daily(df):\n",
        "    dfi = df.copy()\n",
        "    if 'date' in dfi.columns:\n",
        "        dfi['date'] = pd.to_datetime(dfi['date'])\n",
        "        dfi = dfi.set_index('date')\n",
        "    idx = dfi.index\n",
        "    dfi['dayofweek'] = idx.dayofweek\n",
        "    dfi['month'] = idx.month\n",
        "    dfi['dayofyear'] = idx.dayofyear\n",
        "    dfi['dow_sin'] = np.sin(2*np.pi*dfi['dayofweek']/7)\n",
        "    dfi['dow_cos'] = np.cos(2*np.pi*dfi['dayofweek']/7)\n",
        "    dfi['month_sin'] = np.sin(2*np.pi*(dfi['month']-1)/12)\n",
        "    dfi['month_cos'] = np.cos(2*np.pi*(dfi['month']-1)/12)\n",
        "    dfi['time_idx'] = np.arange(len(dfi))\n",
        "    return dfi\n",
        "\n",
        "def winsorize_series(series, lower_q=0.01, upper_q=0.99):\n",
        "    low = series.quantile(lower_q)\n",
        "    high = series.quantile(upper_q)\n",
        "    return series.clip(low, high)\n",
        "\n",
        "def make_windows_from_df(df, feature_cols, target_col, lookback, horizon=1):\n",
        "    arr = df[feature_cols + [target_col]].values\n",
        "    T = arr.shape[0]\n",
        "    n_feat = len(feature_cols)\n",
        "    last_start = T - (lookback + horizon)\n",
        "    if last_start < 0:\n",
        "        return np.empty((0, lookback, n_feat)), np.empty((0, 1))\n",
        "    Xs, ys = [], []\n",
        "    for s in range(0, last_start+1):\n",
        "        Xs.append(arr[s: s+lookback, :n_feat])\n",
        "        ys.append(arr[s+lookback+horizon-1, n_feat])\n",
        "    return np.array(Xs, dtype=np.float32), np.array(ys, dtype=np.float32).reshape(-1,1)\n",
        "\n",
        "def rolling_origin_splits(n_time, n_splits=N_CV_SPLITS, val_block_size=VAL_BLOCK_SIZE, gap=GAP, initial_train_size=None):\n",
        "    if initial_train_size is None:\n",
        "        initial_train_size = n_time - n_splits * val_block_size - gap * n_splits\n",
        "    for i in range(n_splits):\n",
        "        train_end = initial_train_size + i * val_block_size\n",
        "        val_start = train_end + gap\n",
        "        val_end = val_start + val_block_size\n",
        "        if val_end > n_time:\n",
        "            break\n",
        "        train_idx = np.arange(0, train_end)\n",
        "        val_idx = np.arange(val_start, val_end)\n",
        "        yield train_idx, val_idx\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return math.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def build_elman(input_shape, hidden_size=32, dropout=0.0):\n",
        "    inp = keras.Input(shape=input_shape)\n",
        "    x = layers.SimpleRNN(hidden_size, activation='tanh', dropout=dropout, return_sequences=False)(inp)\n",
        "    out = layers.Dense(1)(x)\n",
        "    model = keras.Model(inp, out)\n",
        "    return model\n",
        "\n",
        "def build_multi(input_shape, hidden_size=32, dropout=0.2):\n",
        "    inp = keras.Input(shape=input_shape)\n",
        "    x = layers.SimpleRNN(hidden_size, activation='tanh', dropout=dropout, return_sequences=True)(inp)\n",
        "    x = layers.SimpleRNN(hidden_size, activation='tanh', dropout=dropout, return_sequences=False)(x)\n",
        "    out = layers.Dense(1)(x)\n",
        "    model = keras.Model(inp, out)\n",
        "    return model\n",
        "\n",
        "class JordanRNN(keras.Model):\n",
        "    def __init__(self, input_size, hidden_size, dropout=0.0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.input_size = input_size\n",
        "        self.rnn_cell = layers.SimpleRNNCell(hidden_size, activation='tanh')\n",
        "        self.dropout = layers.Dropout(dropout)\n",
        "        self.fc = layers.Dense(1)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        batch = tf.shape(x)[0]\n",
        "        h = tf.zeros((batch, self.hidden_size))\n",
        "        y_prev = tf.zeros((batch, 1))\n",
        "        for t in range(x.shape[1]):\n",
        "            xt = x[:, t, :]\n",
        "            inp_t = tf.concat([xt, y_prev], axis=1)\n",
        "            out_cell, [h] = self.rnn_cell(inp_t, [h])\n",
        "            if training:\n",
        "                h = self.dropout(h, training=training)\n",
        "            y_prev = self.fc(h)\n",
        "        return y_prev\n",
        "\n",
        "def train_model_keras(model, X_train, y_train, X_val, y_val, cfg):\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=cfg['lr']), loss='mse', metrics=['mae'])\n",
        "    es = keras.callbacks.EarlyStopping(patience=cfg['patience'], restore_best_weights=True, verbose=0)\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=cfg['epochs'],\n",
        "                        batch_size=cfg['batch_size'], callbacks=[es], verbose=cfg.get('verbose', 0))\n",
        "    return model, history\n",
        "\n",
        "def evaluate_model_keras(model, X, y, scaler_y=None):\n",
        "    preds = model.predict(X, batch_size=256)\n",
        "    if scaler_y is not None:\n",
        "        preds_inv = scaler_y.inverse_transform(preds)\n",
        "        y_inv = scaler_y.inverse_transform(y)\n",
        "    else:\n",
        "        preds_inv = preds\n",
        "        y_inv = y\n",
        "    rmse_val = rmse(y_inv, preds_inv)\n",
        "    mae_val = mean_absolute_error(y_inv, preds_inv)\n",
        "    return y_inv, preds_inv, rmse_val, mae_val\n",
        "\n",
        "def pipeline_msft(filepath):\n",
        "    df = read_msft_csv(filepath)\n",
        "\n",
        "    target = TARGET_COL.lower()\n",
        "    base_feats = [c for c in FEATURE_COLS_BASE if c in df.columns]\n",
        "\n",
        "\n",
        "    n = len(df)\n",
        "    test_n = max(1, int(np.floor(n * TEST_FRAC)))\n",
        "    trainval_df = df.iloc[:-test_n].copy()\n",
        "    test_df = df.iloc[-test_n:].copy()\n",
        "\n",
        "    print(\"\\nStationarity on raw Close (train+val):\")\n",
        "    summary = stationarity_report_df(df, exclude_cols=['Date'])\n",
        "    print(summary)\n",
        "    trainval_df['logret'] = np.log(trainval_df[target]).diff()\n",
        "\n",
        "    n_time = len(trainval_df)\n",
        "    combos = list(itertools.product(HYPERPARAM_GRID['lookback'], HYPERPARAM_GRID['hidden'], HYPERPARAM_GRID['lr'], HYPERPARAM_GRID['dropout']))\n",
        "    results = {}\n",
        "\n",
        "    for model_type in ['Elman','Jordan','Multi']:\n",
        "        print(f\"\\n=== Model: {model_type} ===\")\n",
        "        combo_scores = []\n",
        "        for (lb, hid, lr, drop) in combos:\n",
        "            fold_rmse_vals = []\n",
        "            for (train_idx, val_idx) in rolling_origin_splits(n_time, n_splits=N_CV_SPLITS, val_block_size=VAL_BLOCK_SIZE, gap=GAP):\n",
        "                train_fold = trainval_df.iloc[train_idx].copy()\n",
        "                val_fold = trainval_df.iloc[val_idx].copy()\n",
        "\n",
        "                train_fold = train_fold.sort_values('date').reset_index(drop=True)\n",
        "                val_fold = val_fold.sort_values('date').reset_index(drop=True)\n",
        "\n",
        "                for df_f in [train_fold, val_fold]:\n",
        "                    df_f['ret'] = df_f[target].pct_change()\n",
        "                    df_f['logret'] = np.log(df_f[target]).diff()\n",
        "                    df_f['ma_5'] = df_f[target].rolling(window=5, min_periods=1).mean()\n",
        "                    df_f['ma_10'] = df_f[target].rolling(window=10, min_periods=1).mean()\n",
        "\n",
        "                low = train_fold[target].quantile(0.01); high = train_fold[target].quantile(0.99)\n",
        "                train_fold[target] = train_fold[target].clip(low, high)\n",
        "                val_fold[target] = val_fold[target].clip(low, high)\n",
        "\n",
        "                train_fold = add_time_features_daily(train_fold)\n",
        "                val_fold = add_time_features_daily(val_fold)\n",
        "\n",
        "                used_feats = []\n",
        "                used_feats += [c for c in base_feats if c in train_fold.columns]\n",
        "                eng_feats = ['ret','logret','ma_5','ma_10','time_idx','dow_sin','dow_cos','month_sin','month_cos']\n",
        "                used_feats += [c for c in eng_feats if c in train_fold.columns]\n",
        "\n",
        "                train_fold[used_feats + [target]] = train_fold[used_feats + [target]].fillna(0)\n",
        "                val_fold[used_feats + [target]] = val_fold[used_feats + [target]].fillna(0)\n",
        "\n",
        "\n",
        "                scaler_X = StandardScaler()\n",
        "                scaler_y = StandardScaler()\n",
        "                numeric_feats = [c for c in used_feats if c in train_fold.columns]\n",
        "                scaler_X.fit(train_fold[numeric_feats])\n",
        "                scaler_y.fit(train_fold[[target]])\n",
        "\n",
        "                train_scaled = train_fold.copy()\n",
        "                val_scaled = val_fold.copy()\n",
        "                train_scaled[numeric_feats] = scaler_X.transform(train_fold[numeric_feats])\n",
        "                val_scaled[numeric_feats] = scaler_X.transform(val_fold[numeric_feats])\n",
        "                train_scaled[[target]] = scaler_y.transform(train_fold[[target]])\n",
        "                val_scaled[[target]] = scaler_y.transform(val_fold[[target]])\n",
        "\n",
        "                X_train, y_train = make_windows_from_df(train_scaled, numeric_feats, target, lookback=lb, horizon=HORIZON)\n",
        "                val_context = pd.concat([train_scaled.tail(lb), val_scaled])\n",
        "                X_val, y_val = make_windows_from_df(val_context, numeric_feats, target, lookback=lb, horizon=HORIZON)\n",
        "\n",
        "                if X_train.shape[0]==0 or X_val.shape[0]==0:\n",
        "                    fold_rmse_vals.append(np.nan)\n",
        "                    continue\n",
        "\n",
        "                input_shape = (lb, X_train.shape[2])\n",
        "                if model_type == 'Elman':\n",
        "                    model = build_elman(input_shape, hidden_size=hid, dropout=drop)\n",
        "                elif model_type == 'Multi':\n",
        "                    model = build_multi(input_shape, hidden_size=hid, dropout=drop)\n",
        "                elif model_type == 'Jordan':\n",
        "                    model = JordanRNN(input_size=X_train.shape[2], hidden_size=hid, dropout=drop)\n",
        "                    model.build((None, lb, X_train.shape[2]))\n",
        "\n",
        "                cfg = {'epochs': N_EPOCHS, 'lr': lr, 'batch_size': BATCH_SIZE, 'patience': PATIENCE, 'verbose': 0}\n",
        "                model, history = train_model_keras(model, X_train, y_train, X_val, y_val, cfg)\n",
        "\n",
        "                y_val_inv, preds_val_inv, val_rmse, val_mae = evaluate_model_keras(model, X_val, y_val, scaler_y=scaler_y)\n",
        "                fold_rmse_vals.append(val_rmse)\n",
        "\n",
        "            fold_rmse_vals = [v for v in fold_rmse_vals if not np.isnan(v)]\n",
        "            mean_cv_rmse = np.mean(fold_rmse_vals) if len(fold_rmse_vals)>0 else np.nan\n",
        "            combo_scores.append({'lookback':lb,'hidden':hid,'lr':lr,'dropout':drop,'mean_cv_rmse':mean_cv_rmse})\n",
        "\n",
        "        combo_scores = [c for c in combo_scores if not np.isnan(c['mean_cv_rmse'])]\n",
        "        best_combo = sorted(combo_scores, key=lambda x: x['mean_cv_rmse'])[0]\n",
        "        print(f\"\\nBest hyperparams for {model_type}: {best_combo}\")\n",
        "        results[model_type] = {'best_combo': best_combo, 'combo_scores': combo_scores}\n",
        "\n",
        "        train_full = trainval_df.copy().sort_values('date').reset_index(drop=True)\n",
        "        test_full  = test_df.copy().sort_values('date').reset_index(drop=True)\n",
        "\n",
        "\n",
        "        for df_f in [train_full, test_full]:\n",
        "            df_f['ret'] = df_f[target].pct_change()\n",
        "            df_f['logret'] = np.log(df_f[target]).diff()\n",
        "            df_f['ma_5'] = df_f[target].rolling(window=5, min_periods=1).mean()\n",
        "            df_f['ma_10'] = df_f[target].rolling(window=10, min_periods=1).mean()\n",
        "\n",
        "        low = train_full[target].quantile(0.01); high = train_full[target].quantile(0.99)\n",
        "        train_full[target] = train_full[target].clip(low, high)\n",
        "        test_full[target]  = test_full[target].clip(low, high)\n",
        "\n",
        "        train_full = add_time_features_daily(train_full)\n",
        "        test_full = add_time_features_daily(test_full)\n",
        "\n",
        "        used_feats = [c for c in base_feats if c in train_full.columns] + [c for c in ['ret','logret','ma_5','ma_10','time_idx','dow_sin','dow_cos','month_sin','month_cos'] if c in train_full.columns]\n",
        "\n",
        "        train_full[used_feats + [target]] = train_full[used_feats + [target]].fillna(0) # or .median() or .mean()\n",
        "        test_full[used_feats + [target]] = test_full[used_feats + [target]].fillna(0)\n",
        "\n",
        "\n",
        "        scaler_X = StandardScaler()\n",
        "        scaler_y = StandardScaler()\n",
        "        scaler_X.fit(train_full[used_feats])\n",
        "        scaler_y.fit(train_full[[target]])\n",
        "        train_scaled = train_full.copy(); test_scaled = test_full.copy()\n",
        "        train_scaled[used_feats] = scaler_X.transform(train_full[used_feats])\n",
        "        test_scaled[used_feats] = scaler_X.transform(test_full[used_feats])\n",
        "        train_scaled[[target]] = scaler_y.transform(train_full[[target]])\n",
        "        test_scaled[[target]] = scaler_y.transform(test_full[[target]])\n",
        "\n",
        "        lb = best_combo['lookback']; hid = best_combo['hidden']; lr = best_combo['lr']; drop = best_combo['dropout']\n",
        "        X_train_all, y_train_all = make_windows_from_df(train_scaled, used_feats, target, lookback=lb, horizon=HORIZON)\n",
        "        test_context = pd.concat([train_scaled.tail(lb), test_scaled])\n",
        "        X_test, y_test = make_windows_from_df(test_context, used_feats, target, lookback=lb, horizon=HORIZON)\n",
        "\n",
        "        if model_type == 'Elman':\n",
        "            final_model = build_elman((lb, X_train_all.shape[2]), hidden_size=hid, dropout=drop)\n",
        "        elif model_type == 'Multi':\n",
        "            final_model = build_multi((lb, X_train_all.shape[2]), hidden_size=hid, dropout=drop)\n",
        "        else:\n",
        "            final_model = JordanRNN(input_size=X_train_all.shape[2], hidden_size=hid, dropout=drop)\n",
        "            final_model.build((None, lb, X_train_all.shape[2]))\n",
        "\n",
        "        cfg = {'epochs': N_EPOCHS, 'lr': lr, 'batch_size': BATCH_SIZE, 'patience': PATIENCE, 'verbose': 1}\n",
        "        final_model, history = train_model_keras(final_model, X_train_all, y_train_all, X_train_all[:0], y_train_all[:0], cfg)\n",
        "\n",
        "        y_true_test, y_pred_test, final_rmse, final_mae = evaluate_model_keras(final_model, X_test, y_test, scaler_y=scaler_y)\n",
        "        print(f\" Final TEST for {model_type}: RMSE={final_rmse:.4f}, MAE={final_mae:.4f}\")\n",
        "\n",
        "        results[model_type].update({\n",
        "            'final_model': final_model,\n",
        "            'scaler_X': scaler_X, 'scaler_y': scaler_y,\n",
        "            'y_true_test': y_true_test, 'y_pred_test': y_pred_test,\n",
        "            'test_rmse': final_rmse, 'test_mae': final_mae,\n",
        "            'history': history.history\n",
        "        })\n",
        "\n",
        "\n",
        "    summary = {}\n",
        "    for m in ['Elman','Jordan','Multi']:\n",
        "        if m in results and 'test_rmse' in results[m]:\n",
        "            summary[m] = {'best_combo': results[m]['best_combo'], 'test_rmse': results[m]['test_rmse'], 'test_mae': results[m]['test_mae']}\n",
        "        else:\n",
        "            summary[m] = {'note': 'no result'}\n",
        "    print(\"\\nSummary of final test metrics:\")\n",
        "    print(summary)\n",
        "    return results\n",
        "\n",
        "results = pipeline_msft(FILE_PATH)"
      ],
      "metadata": {
        "id": "kug0LXp0F7lT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}