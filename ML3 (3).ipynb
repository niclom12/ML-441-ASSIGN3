{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hGluL50RhGX"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJQCXSL3WZpP"
      },
      "outputs": [],
      "source": [
        "path = kagglehub.dataset_download(\"sumanthvrao/daily-climate-time-series-data\")\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJ0h8rYoX3K9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "print(os.listdir(path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08b93428"
      },
      "outputs": [],
      "source": [
        "train_file_name = \"DailyDelhiClimateTrain.csv\"\n",
        "test_file_name = \"DailyDelhiClimateTest.csv\"\n",
        "\n",
        "train_file_path = os.path.join(path, train_file_name)\n",
        "test_file_path = os.path.join(path, test_file_name)\n",
        "\n",
        "train_df = pd.read_csv(train_file_path)\n",
        "test_df = pd.read_csv(test_file_path)\n",
        "\n",
        "print(\"Train DataFrame head:\")\n",
        "display(train_df.head())\n",
        "print(\"\\nTest DataFrame head:\")\n",
        "display(test_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import math\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "\n",
        "train_file_name = \"DailyDelhiClimateTrain.csv\"\n",
        "test_file_name = \"DailyDelhiClimateTest.csv\"\n",
        "\n",
        "TRAIN_CSV_PATH = os.path.join(path, train_file_name)\n",
        "TEST_CSV_PATH = os.path.join(path, test_file_name)\n",
        "\n",
        "TIME_COLUMN = \"date\"\n",
        "TARGET_COLUMN = \"meantemp\"\n",
        "FEATURE_COLUMNS = [\"humidity\", \"wind_speed\", \"meanpressure\"]\n",
        "\n",
        "\n",
        "HYPERPARAMETERS_TO_TEST = {\n",
        "    'lookback': [14, 30],\n",
        "    'hidden_units': [32],\n",
        "}\n",
        "N_EPOCHS = 50\n",
        "PATIENCE = 10\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "N_CV_SPLITS = 3\n",
        "VALIDATION_BLOCK_SIZE = 90\n",
        "\n",
        "\n",
        "def load_data(train_path, test_path, time_col):\n",
        "    train_df = pd.read_csv(train_path, parse_dates=[time_col], index_col=time_col)\n",
        "    test_df = pd.read_csv(test_path, parse_dates=[time_col], index_col=time_col)\n",
        "    return train_df.sort_index(), test_df.sort_index()\n",
        "\n",
        "def create_windows(data, lookback, target_column_index):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - lookback):\n",
        "        X.append(data[i:(i + lookback)])\n",
        "        y.append(data[i + lookback, target_column_index])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def get_cv_splits(data_length, n_splits, val_size):\n",
        "    initial_train_size = data_length - (n_splits * val_size)\n",
        "    for i in range(n_splits):\n",
        "        train_end = initial_train_size + i * val_size\n",
        "        val_end = train_end + val_size\n",
        "\n",
        "        train_indices = range(0, train_end)\n",
        "        val_indices = range(train_end, val_end)\n",
        "        yield train_indices, val_indices\n",
        "\n",
        "\n",
        "\n",
        "def build_elman_model(input_shape, hidden_units):\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=input_shape),\n",
        "        layers.SimpleRNN(hidden_units, activation='tanh'),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_multi_layer_model(input_shape, hidden_units):\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=input_shape),\n",
        "        layers.SimpleRNN(hidden_units, activation='tanh', return_sequences=True),\n",
        "        layers.SimpleRNN(hidden_units, activation='tanh'),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "class JordanRNN(keras.Model):\n",
        "    def __init__(self, hidden_units):\n",
        "        super().__init__()\n",
        "        self.rnn_cell = layers.SimpleRNNCell(hidden_units, activation='tanh')\n",
        "        # The final output layer.\n",
        "        self.dense_output = layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        hidden_state = tf.zeros((batch_size, self.rnn_cell.units))\n",
        "        previous_output = tf.zeros((batch_size, 1))\n",
        "\n",
        "        for t in range(inputs.shape[1]):\n",
        "            current_features = inputs[:, t, :]\n",
        "            # Combine current features with the previous output.\n",
        "            combined_input = tf.concat([current_features, previous_output], axis=1)\n",
        "            _, [hidden_state] = self.rnn_cell(combined_input, [hidden_state])\n",
        "\n",
        "            # Calculate the output used in the next iteration.\n",
        "            previous_output = self.dense_output(hidden_state)\n",
        "\n",
        "        return previous_output\n",
        "\n",
        "\n",
        "def main():\n",
        "    train_df, test_df = load_data(TRAIN_CSV_PATH, TEST_CSV_PATH, TIME_COLUMN)\n",
        "    train_df = train_df.ffill()\n",
        "    test_df = test_df.ffill()\n",
        "\n",
        "    models_to_evaluate = {\n",
        "        'Elman': build_elman_model,\n",
        "        'Multi-Layer': build_multi_layer_model,\n",
        "        'Jordan': JordanRNN\n",
        "    }\n",
        "\n",
        "    final_results = {}\n",
        "\n",
        "    for model_name, model_builder in models_to_evaluate.items():\n",
        "        best_params = {}\n",
        "        best_avg_rmse = float('inf')\n",
        "        for lookback in HYPERPARAMETERS_TO_TEST['lookback']:\n",
        "            for hidden_units in HYPERPARAMETERS_TO_TEST['hidden_units']:\n",
        "                print(f\"  Testing params: lookback={lookback}, hidden_units={hidden_units}\")\n",
        "                fold_rmses = []\n",
        "\n",
        "                cv_splits = get_cv_splits(len(train_df), N_CV_SPLITS, VALIDATION_BLOCK_SIZE)\n",
        "\n",
        "                for train_idx, val_idx in cv_splits:\n",
        "                    train_fold = train_df.iloc[train_idx]\n",
        "                    val_fold = train_df.iloc[val_idx]\n",
        "\n",
        "                    all_cols = [TARGET_COLUMN] + FEATURE_COLUMNS\n",
        "\n",
        "                    scaler_X = StandardScaler()\n",
        "                    scaler_y = StandardScaler()\n",
        "\n",
        "                    train_features_scaled = scaler_X.fit_transform(train_fold[FEATURE_COLUMNS])\n",
        "                    train_target_scaled = scaler_y.fit_transform(train_fold[[TARGET_COLUMN]])\n",
        "\n",
        "                    val_features_scaled = scaler_X.transform(val_fold[FEATURE_COLUMNS])\n",
        "                    val_target_scaled = scaler_y.transform(val_fold[[TARGET_COLUMN]])\n",
        "\n",
        "                    train_scaled_data = np.hstack([train_target_scaled, train_features_scaled])\n",
        "\n",
        "                    validation_context = np.vstack([\n",
        "                        train_scaled_data[-lookback:],\n",
        "                        np.hstack([val_target_scaled, val_features_scaled])\n",
        "                    ])\n",
        "\n",
        "                    X_train, y_train = create_windows(train_scaled_data, lookback, target_column_index=0)\n",
        "                    X_val, y_val = create_windows(validation_context, lookback, target_column_index=0)\n",
        "\n",
        "                    if X_train.shape[0] == 0 or X_val.shape[0] == 0:\n",
        "                        continue # Skip if a not enough data\n",
        "\n",
        "                    n_features = X_train.shape[2]\n",
        "                    input_shape = (lookback, n_features)\n",
        "\n",
        "                    if model_name == 'Jordan':\n",
        "                        model = model_builder(hidden_units=hidden_units)\n",
        "                    else:\n",
        "                        model = model_builder(input_shape=input_shape, hidden_units=hidden_units)\n",
        "\n",
        "                    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "                    early_stopping = keras.callbacks.EarlyStopping(\n",
        "                        monitor='val_loss', patience=PATIENCE, restore_best_weights=True\n",
        "                    )\n",
        "\n",
        "                    model.fit(X_train, y_train,\n",
        "                              validation_data=(X_val, y_val),\n",
        "                              epochs=N_EPOCHS,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              callbacks=[early_stopping],\n",
        "                              verbose=0)\n",
        "\n",
        "                    predictions_scaled = model.predict(X_val, verbose=0)\n",
        "                    predictions_unscaled = scaler_y.inverse_transform(predictions_scaled)\n",
        "                    true_values_unscaled = scaler_y.inverse_transform(y_val.reshape(-1, 1))\n",
        "\n",
        "                    fold_rmse = math.sqrt(mean_squared_error(true_values_unscaled, predictions_unscaled))\n",
        "                    fold_rmses.append(fold_rmse)\n",
        "\n",
        "                avg_rmse = np.mean(fold_rmses)\n",
        "                print(f\"Average CV RMSE: {avg_rmse:.4f}\")\n",
        "                if avg_rmse < best_avg_rmse:\n",
        "                    best_avg_rmse = avg_rmse\n",
        "                    best_params = {'lookback': lookback, 'hidden_units': hidden_units}\n",
        "\n",
        "        print(f\"\\n  Best parameters found for {model_name}: {best_params} (RMSE: {best_avg_rmse:.4f})\")\n",
        "\n",
        "\n",
        "        final_lookback = best_params['lookback']\n",
        "        final_hidden_units = best_params['hidden_units']\n",
        "\n",
        "        scaler_X_final = StandardScaler()\n",
        "        scaler_y_final = StandardScaler()\n",
        "\n",
        "        train_features_scaled_final = scaler_X_final.fit_transform(train_df[FEATURE_COLUMNS])\n",
        "        train_target_scaled_final = scaler_y_final.fit_transform(train_df[[TARGET_COLUMN]])\n",
        "\n",
        "        test_features_scaled_final = scaler_X_final.transform(test_df[FEATURE_COLUMNS])\n",
        "        test_target_scaled_final = scaler_y_final.transform(test_df[[TARGET_COLUMN]])\n",
        "\n",
        "        # Combine data for windowing\n",
        "        train_scaled_final = np.hstack([train_target_scaled_final, train_features_scaled_final])\n",
        "\n",
        "        test_context_final = np.vstack([\n",
        "            train_scaled_final[-final_lookback:],\n",
        "            np.hstack([test_target_scaled_final, test_features_scaled_final])\n",
        "        ])\n",
        "\n",
        "        X_train_final, y_train_final = create_windows(train_scaled_final, final_lookback, 0)\n",
        "        X_test_final, y_test_final = create_windows(test_context_final, final_lookback, 0)\n",
        "\n",
        "        n_features_final = X_train_final.shape[2]\n",
        "        input_shape_final = (final_lookback, n_features_final)\n",
        "\n",
        "        if model_name == 'Jordan':\n",
        "            final_model = model_builder(hidden_units=final_hidden_units)\n",
        "        else:\n",
        "            final_model = model_builder(input_shape=input_shape_final, hidden_units=final_hidden_units)\n",
        "\n",
        "        final_model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "        final_model.fit(X_train_final, y_train_final,\n",
        "                        epochs=N_EPOCHS,\n",
        "                        batch_size=BATCH_SIZE,\n",
        "                        verbose=0)\n",
        "\n",
        "        final_predictions_scaled = final_model.predict(X_test_final, verbose=0)\n",
        "        final_predictions_unscaled = scaler_y_final.inverse_transform(final_predictions_scaled)\n",
        "        final_true_values_unscaled = scaler_y_final.inverse_transform(y_test_final.reshape(-1, 1))\n",
        "\n",
        "        final_test_rmse = math.sqrt(mean_squared_error(final_true_values_unscaled, final_predictions_unscaled))\n",
        "        print(f\"  -> Final Test RMSE for {model_name}: {final_test_rmse:.4f}\")\n",
        "\n",
        "        final_results[model_name] = {\n",
        "            'best_params': best_params,\n",
        "            'test_rmse': final_test_rmse,\n",
        "            'predictions': final_predictions_unscaled.flatten(),\n",
        "            'true_values': final_true_values_unscaled.flatten()\n",
        "        }\n",
        "\n",
        "    print(f\"\\n{'='*30}\\nFINAL RESULTS SUMMARY\\n{'='*30}\")\n",
        "    best_overall_model = None\n",
        "    lowest_rmse = float('inf')\n",
        "\n",
        "    for model_name, result in final_results.items():\n",
        "        print(f\"Model: {model_name}\")\n",
        "        print(f\"  - Best Hyperparameters: {result['best_params']}\")\n",
        "        print(f\"  - Final Test RMSE: {result['test_rmse']:.4f}\")\n",
        "        if result['test_rmse'] < lowest_rmse:\n",
        "            lowest_rmse = result['test_rmse']\n",
        "            best_overall_model = model_name\n",
        "\n",
        "    print(f\"\\n Overall Best Performing Model: {best_overall_model} (RMSE: {lowest_rmse:.4f})\")\n",
        "\n",
        "main()\n",
        "\n",
        "# Commenrt on the custom jordan implementation\n",
        "# Comment on the steps taken to prevent overfitting\n",
        "# Describe Cross validation approach used\n",
        "# Describe data preprocessing and justifications\n",
        "# Describe optimiser and loss function used\n",
        "# Carefully define the empirical process that you have followed, and describe this process in your report.The process has to include settings for all hyperparameters, neural network architecture, performance measures, and the process followed to determine which simple recurrent neural network performed best for each of the datasets."
      ],
      "metadata": {
        "id": "qNF70SJVNorb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from statsmodels.tsa.stattools import adfuller, kpss\n",
        "\n",
        "def adf_test(series, signif=0.05):\n",
        "    result = adfuller(series.dropna(), autolag='AIC')\n",
        "    stat, pval, usedlag, nobs, crit_values, icbest = result\n",
        "    stationary = pval < signif\n",
        "    return {'stat': stat, 'pval': pval, 'usedlag': usedlag, 'nobs': nobs,\n",
        "            'crit': crit_values, 'stationary': stationary}\n",
        "\n",
        "def kpss_test(series, regression='c', signif=0.05):\n",
        "    stat, pval, nlags, crit_values = kpss(series.dropna(), regression=regression, nlags=\"auto\")\n",
        "    stationary = pval > signif\n",
        "    return {'stat': stat, 'pval': pval, 'nlags': nlags, 'crit': crit_values, 'stationary': stationary}\n",
        "\n",
        "def check_stationarity(df, columns=None, signif=0.05, verbose=True):\n",
        "    df_copy = df.copy()\n",
        "    if not isinstance(df_copy.index, pd.DatetimeIndex):\n",
        "        for alt in ['date', 'time', 'timestamp']:\n",
        "            if alt in df_copy.columns:\n",
        "                df_copy[alt] = pd.to_datetime(df_copy[alt], errors='coerce')\n",
        "                df_copy = df_copy.set_index(alt)\n",
        "                break\n",
        "\n",
        "    if columns is None:\n",
        "        columns = df_copy.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "    results = {}\n",
        "    for col in columns:\n",
        "        series = df_copy[col]\n",
        "        adf_res = adf_test(series, signif)\n",
        "        kpss_res = kpss_test(series, signif=signif)\n",
        "        if adf_res['stationary'] and kpss_res['stationary']:\n",
        "            conclusion = 'Stationary'\n",
        "        elif not adf_res['stationary'] and not kpss_res['stationary']:\n",
        "            conclusion = 'Non-stationary'\n",
        "        else:\n",
        "            conclusion = 'Mixed/Borderline'\n",
        "        results[col] = {'ADF': adf_res, 'KPSS': kpss_res, 'conclusion': conclusion}\n",
        "        if verbose:\n",
        "            print(f\"\\nColumn: {col}\")\n",
        "            print(f\"  ADF: stat={adf_res['stat']:.4f}, p={adf_res['pval']:.4f}, stationary={adf_res['stationary']}\")\n",
        "            print(f\"  KPSS: stat={kpss_res['stat']:.4f}, p={kpss_res['pval']:.4f}, stationary={kpss_res['stationary']}\")\n",
        "            print(f\"  => Overall conclusion: {conclusion}\")\n",
        "    return results\n",
        "\n",
        "\n",
        "results = check_stationarity(train_df)\n"
      ],
      "metadata": {
        "id": "bNRYMIDHnaic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = kagglehub.dataset_download(\"gabrielsantello/airline-baggage-complaints-time-series-dataset\")\n",
        "file_name = \"baggagecomplaints.csv\"\n",
        "file_path = os.path.join(path, file_name)\n",
        "df = pd.read_csv(file_path)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "8aMRXC_9r1t5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns.tolist())"
      ],
      "metadata": {
        "id": "pelztNoBtZcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "FILE_PATH = os.path.join(path, file_name)\n",
        "TARGET_COLUMN = \"baggage\"\n",
        "NUMERIC_FEATURES = [\"scheduled\", \"cancelled\", \"enplaned\"]\n",
        "CATEGORICAL_FEATURE = \"airline\"\n",
        "\n",
        "HYPERPARAMETERS_TO_TEST = {\n",
        "    'lookback': [24],\n",
        "    'hidden_units': [32],\n",
        "    'learning_rate': [0.001],\n",
        "    'dropout': [0.2],\n",
        "}\n",
        "\n",
        "N_EPOCHS = 20\n",
        "PATIENCE = 10\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "TEST_SET_FRACTION = 0.2\n",
        "N_CV_SPLITS = 3\n",
        "VALIDATION_BLOCK_SIZE = 24\n",
        "\n",
        "def load_and_prepare_data(filepath):\n",
        "    df = pd.read_csv(filepath)\n",
        "    df.columns = df.columns.str.strip().str.lower()\n",
        "    df['date'] = pd.to_datetime(df['date'], format=\"%m/%Y\")\n",
        "    return df.set_index('date').sort_index()\n",
        "\n",
        "def add_time_features(df):\n",
        "    df_copy = df.copy()\n",
        "    df_copy['month'] = df_copy.index.month\n",
        "    df_copy['month_sin'] = np.sin(2 * np.pi * (df_copy['month'] - 1) / 12)\n",
        "    df_copy['month_cos'] = np.cos(2 * np.pi * (df_copy['month'] - 1) / 12)\n",
        "    df_copy['time_idx'] = np.arange(len(df_copy))\n",
        "    return df_copy\n",
        "\n",
        "def create_windows(data, lookback):\n",
        "    X, y = [], []\n",
        "    target_column_index = 0\n",
        "    for i in range(len(data) - lookback):\n",
        "        X.append(data[i:(i + lookback)])\n",
        "        y.append(data[i + lookback, target_column_index])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def get_cv_splits(data_length, n_splits, val_size):\n",
        "    initial_train_size = data_length - (n_splits * val_size)\n",
        "    for i in range(n_splits):\n",
        "        train_end = initial_train_size + i * val_size\n",
        "        val_end = train_end + val_size\n",
        "        yield range(0, train_end), range(train_end, val_end)\n",
        "\n",
        "\n",
        "def build_elman_model(input_shape, hidden_units, dropout):\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=input_shape),\n",
        "        layers.SimpleRNN(hidden_units, activation='tanh', dropout=dropout),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_multi_layer_model(input_shape, hidden_units, dropout):\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=input_shape),\n",
        "        layers.SimpleRNN(hidden_units, activation='tanh', return_sequences=True, dropout=dropout),\n",
        "        layers.SimpleRNN(hidden_units, activation='tanh', dropout=dropout),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "class JordanRNN(keras.Model):\n",
        "    def __init__(self, hidden_units):\n",
        "        super().__init__()\n",
        "        self.rnn_cell = layers.SimpleRNNCell(hidden_units, activation='tanh')\n",
        "        # The final output layer.\n",
        "        self.dense_output = layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        hidden_state = tf.zeros((batch_size, self.rnn_cell.units))\n",
        "        previous_output = tf.zeros((batch_size, 1))\n",
        "\n",
        "        for t in range(inputs.shape[1]):\n",
        "            current_features = inputs[:, t, :]\n",
        "            # Combine current features with the previous output.\n",
        "            combined_input = tf.concat([current_features, previous_output], axis=1)\n",
        "            _, [hidden_state] = self.rnn_cell(combined_input, [hidden_state])\n",
        "\n",
        "            # Calculate the output used in the next iteration.\n",
        "            previous_output = self.dense_output(hidden_state)\n",
        "\n",
        "        return previous_output\n",
        "\n",
        "\n",
        "full_df = load_and_prepare_data(FILE_PATH)\n",
        "full_df = full_df.ffill().bfill()\n",
        "\n",
        "test_size = int(len(full_df) * TEST_SET_FRACTION)\n",
        "trainval_df = full_df.iloc[:-test_size].copy()\n",
        "test_df = full_df.iloc[-test_size:].copy()\n",
        "results = check_stationarity(trainval_df)\n",
        "\n",
        "models_to_evaluate = {\n",
        "    'Elman': build_elman_model,\n",
        "    'Multi-Layer': build_multi_layer_model,\n",
        "    'Jordan': JordanRNN\n",
        "}\n",
        "param_grid = HYPERPARAMETERS_TO_TEST\n",
        "param_combos = list(itertools.product(\n",
        "    param_grid['lookback'], param_grid['hidden_units'],\n",
        "    param_grid['learning_rate'], param_grid['dropout']\n",
        "))\n",
        "\n",
        "final_results = {}\n",
        "\n",
        "for model_name, model_builder in models_to_evaluate.items():\n",
        "    print(f\"\\n{'='*40}\\nStarting evaluation for: {model_name}\\n{'='*40}\")\n",
        "\n",
        "    best_params = {}\n",
        "    best_avg_rmse = float('inf')\n",
        "\n",
        "    for lookback, hidden_units, lr, dropout in param_combos:\n",
        "        print(f\"  Testing params: lookback={lookback}, hidden={hidden_units}, lr={lr}, dropout={dropout}\")\n",
        "        fold_rmses = []\n",
        "        cv_splits = get_cv_splits(len(trainval_df), N_CV_SPLITS, VALIDATION_BLOCK_SIZE)\n",
        "\n",
        "        for train_idx, val_idx in cv_splits:\n",
        "            train_fold = trainval_df.iloc[train_idx].copy()\n",
        "            val_fold = trainval_df.iloc[val_idx].copy()\n",
        "\n",
        "            train_fold = add_time_features(train_fold)\n",
        "            val_fold = add_time_features(val_fold)\n",
        "\n",
        "            train_dummies = pd.get_dummies(train_fold[CATEGORICAL_FEATURE], prefix='air')\n",
        "            val_dummies = pd.get_dummies(val_fold[CATEGORICAL_FEATURE], prefix='air')\n",
        "            val_dummies = val_dummies.reindex(columns=train_dummies.columns, fill_value=0)\n",
        "            train_fold = pd.concat([train_fold, train_dummies], axis=1)\n",
        "            val_fold = pd.concat([val_fold, val_dummies], axis=1)\n",
        "\n",
        "            time_features = ['month_sin', 'month_cos', 'time_idx']\n",
        "            all_features = NUMERIC_FEATURES + time_features + list(train_dummies.columns)\n",
        "\n",
        "            scaler_X = StandardScaler()\n",
        "            scaler_y = StandardScaler()\n",
        "\n",
        "            train_fold[all_features] = scaler_X.fit_transform(train_fold[all_features])\n",
        "            train_fold[[TARGET_COLUMN]] = scaler_y.fit_transform(train_fold[[TARGET_COLUMN]])\n",
        "            val_fold[all_features] = scaler_X.transform(val_fold[all_features])\n",
        "            val_fold[[TARGET_COLUMN]] = scaler_y.transform(val_fold[[TARGET_COLUMN]])\n",
        "\n",
        "            train_data_for_windows = train_fold[[TARGET_COLUMN] + all_features].values\n",
        "            val_context = pd.concat([train_fold.tail(lookback), val_fold])\n",
        "            val_data_for_windows = val_context[[TARGET_COLUMN] + all_features].values\n",
        "\n",
        "            X_train, y_train = create_windows(train_data_for_windows, lookback)\n",
        "            X_val, y_val = create_windows(val_data_for_windows, lookback)\n",
        "\n",
        "            if X_train.shape[0] == 0 or X_val.shape[0] == 0: continue\n",
        "\n",
        "            input_shape = (lookback, X_train.shape[2])\n",
        "            if model_name == 'Elman':\n",
        "                model = model_builder(input_shape=input_shape, hidden_units=hidden_units, dropout=dropout)\n",
        "            elif model_name == 'Multi-Layer':\n",
        "                model = model_builder(input_shape=input_shape, hidden_units=hidden_units, dropout=dropout)\n",
        "            elif model_name == 'Jordan':\n",
        "                model = model_builder(hidden_units=hidden_units)\n",
        "                model.build(input_shape)\n",
        "\n",
        "            model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), loss='mse')\n",
        "            early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
        "\n",
        "            model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
        "                      epochs=N_EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping], verbose=0)\n",
        "\n",
        "            preds_scaled = model.predict(X_val, verbose=0)\n",
        "            preds_unscaled = scaler_y.inverse_transform(preds_scaled)\n",
        "            true_unscaled = scaler_y.inverse_transform(y_val.reshape(-1, 1))\n",
        "            fold_rmse = math.sqrt(mean_squared_error(true_unscaled, preds_unscaled))\n",
        "            fold_rmses.append(fold_rmse)\n",
        "\n",
        "        avg_rmse = np.mean(fold_rmses) if fold_rmses else float('inf')\n",
        "        if avg_rmse < best_avg_rmse:\n",
        "            best_avg_rmse = avg_rmse\n",
        "            best_params = {'lookback': lookback, 'hidden_units': hidden_units, 'lr': lr, 'dropout': dropout}\n",
        "\n",
        "    print(f\"\\n  Best parameters for {model_name}: {best_params} (Avg. CV RMSE: {best_avg_rmse:.2f})\")\n",
        "\n",
        "\n",
        "    train_final = add_time_features(trainval_df)\n",
        "    test_final = add_time_features(test_df)\n",
        "\n",
        "    train_dummies_final = pd.get_dummies(train_final[CATEGORICAL_FEATURE], prefix='air')\n",
        "    test_dummies_final = pd.get_dummies(test_final[CATEGORICAL_FEATURE], prefix='air')\n",
        "    test_dummies_final = test_dummies_final.reindex(columns=train_dummies_final.columns, fill_value=0)\n",
        "    train_final = pd.concat([train_final, train_dummies_final], axis=1)\n",
        "    test_final = pd.concat([test_final, test_dummies_final], axis=1)\n",
        "\n",
        "    time_features_final = ['month_sin', 'month_cos', 'time_idx']\n",
        "    all_features_final = NUMERIC_FEATURES + time_features_final + list(train_dummies_final.columns)\n",
        "\n",
        "    scaler_X_final = StandardScaler()\n",
        "    scaler_y_final = StandardScaler()\n",
        "\n",
        "    train_final[all_features_final] = scaler_X_final.fit_transform(train_final[all_features_final])\n",
        "    train_final[[TARGET_COLUMN]] = scaler_y_final.fit_transform(train_final[[TARGET_COLUMN]])\n",
        "    test_final[all_features_final] = scaler_X_final.transform(test_final[all_features_final])\n",
        "    test_final[[TARGET_COLUMN]] = scaler_y_final.transform(test_final[[TARGET_COLUMN]])\n",
        "\n",
        "    train_data_final = train_final[[TARGET_COLUMN] + all_features_final].values\n",
        "    test_context_final = pd.concat([train_final.tail(best_params['lookback']), test_final])\n",
        "    test_data_final = test_context_final[[TARGET_COLUMN] + all_features_final].values\n",
        "\n",
        "    X_train_final, y_train_final = create_windows(train_data_final, best_params['lookback'])\n",
        "    X_test_final, y_test_final = create_windows(test_data_final, best_params['lookback'])\n",
        "\n",
        "    final_input_shape = (best_params['lookback'], X_train_final.shape[2])\n",
        "    if model_name == 'Elman':\n",
        "        final_model = model_builder(input_shape=final_input_shape, hidden_units=best_params['hidden_units'], dropout=best_params['dropout'])\n",
        "    elif model_name == 'Multi-Layer':\n",
        "        final_model = model_builder(input_shape=final_input_shape, hidden_units=best_params['hidden_units'], dropout=best_params['dropout'])\n",
        "    elif model_name == 'Jordan':\n",
        "        final_model = model_builder(hidden_units=best_params['hidden_units'])\n",
        "        final_model.build(final_input_shape)\n",
        "\n",
        "    final_model.compile(optimizer=keras.optimizers.Adam(learning_rate=best_params['lr']), loss='mse')\n",
        "\n",
        "    final_model.fit(X_train_final, y_train_final, epochs=N_EPOCHS, batch_size=BATCH_SIZE, verbose=0)\n",
        "\n",
        "    final_preds_scaled = final_model.predict(X_test_final, verbose=0)\n",
        "    final_preds_unscaled = scaler_y_final.inverse_transform(final_preds_scaled)\n",
        "    final_true_unscaled = scaler_y_final.inverse_transform(y_test_final.reshape(-1, 1))\n",
        "\n",
        "    final_test_rmse = math.sqrt(mean_squared_error(final_true_unscaled, final_preds_unscaled))\n",
        "    print(f\"  -> Final Test Set RMSE for {model_name}: {final_test_rmse:.2f}\")\n",
        "\n",
        "    final_results[model_name] = {\n",
        "        'best_params': best_params,\n",
        "        'test_rmse': final_test_rmse,\n",
        "        'predictions': final_preds_unscaled.flatten(),\n",
        "        'true_values': final_true_unscaled.flatten(),\n",
        "        'test_dates': test_df.index[best_params['lookback']:]\n",
        "    }\n",
        "\n",
        "print(f\"\\n{'='*40}\\nFINAL RESULTS SUMMARY\\n{'='*40}\")\n",
        "best_overall_model_name = None\n",
        "lowest_rmse = float('inf')\n",
        "\n",
        "for model_name, result in final_results.items():\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"  - Best Hyperparameters: {result['best_params']}\")\n",
        "    print(f\"  - Final Test RMSE: {result['test_rmse']:.2f}\")\n",
        "    if result['test_rmse'] < lowest_rmse:\n",
        "        lowest_rmse = result['test_rmse']\n",
        "        best_overall_model_name = model_name\n",
        "\n",
        "print(f\"\\n Overall Best Performing Model: {best_overall_model_name} (RMSE: {lowest_rmse:.2f})\")\n"
      ],
      "metadata": {
        "id": "wQeGeK5oQojd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = kagglehub.dataset_download(\"ujjwalchowdhury/walmartcleaned\")\n",
        "print(os.listdir(path))\n",
        "file_name = \"walmart_cleaned.csv\"\n",
        "file_path = os.path.join(path, file_name)\n",
        "df = pd.read_csv(file_path)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "eW3aQyiQxYa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "JWogsxvszF1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, math, itertools, numpy as np, pandas as pd, tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from itertools import product\n",
        "\n",
        "print(\"\\n--- Walmart Sales Forecasting Pipeline ---\")\n",
        "\n",
        "FILE_PATH = os.path.join(path, file_name)\n",
        "STORE_ID, DEPT_ID = 1, 1\n",
        "TARGET_COLUMN = \"weekly_sales\"\n",
        "NUMERIC_FEATURES = [\"temperature\", \"fuel_price\", \"markdown1\", \"markdown2\", \"markdown3\", \"markdown4\", \"markdown5\", \"cpi\", \"unemployment\", \"size\"]\n",
        "CATEGORICAL_FEATURES = [\"isholiday\", \"type\"]\n",
        "\n",
        "HYPERPARAMS = {'lookback': [26], 'hidden_units': [32], 'lr': [1e-3], 'dropout': [0.2]}\n",
        "EPOCHS, BATCH_SIZE, PATIENCE = 20, 32, 5\n",
        "CV_SPLITS, VAL_BLOCK_SIZE, TEST_FRAC = 3, 20, 0.2\n",
        "\n",
        "def load_and_prepare(filepath):\n",
        "    df = pd.read_csv(filepath)\n",
        "    df.columns = df.columns.str.strip().str.lower()\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "    return df.set_index(\"date\").sort_index()\n",
        "\n",
        "def add_time_features(df):\n",
        "    df = df.copy()\n",
        "    df[\"week\"] = df.index.isocalendar().week.astype(int)\n",
        "    df[\"week_sin\"] = np.sin(2*np.pi*(df[\"week\"]-1)/52)\n",
        "    df[\"week_cos\"] = np.cos(2*np.pi*(df[\"week\"]-1)/52)\n",
        "    df[\"time_idx\"] = np.arange(len(df))\n",
        "    return df\n",
        "\n",
        "def make_windows(X, y, lookback):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - lookback):\n",
        "        Xs.append(X[i:i+lookback])\n",
        "        ys.append(y[i+lookback])\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "def get_cv_splits(n, n_splits, val_size):\n",
        "    start = n - n_splits*val_size\n",
        "    for i in range(n_splits):\n",
        "        yield range(0, start + i*val_size), range(start + i*val_size, start + (i+1)*val_size)\n",
        "\n",
        "\n",
        "def build_elman(input_shape, hidden, dropout):\n",
        "    return keras.Sequential([\n",
        "        layers.Input(shape=input_shape),\n",
        "        layers.SimpleRNN(hidden, activation='tanh', dropout=dropout),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "def build_multi(input_shape, hidden, dropout):\n",
        "    return keras.Sequential([\n",
        "        layers.Input(shape=input_shape),\n",
        "        layers.SimpleRNN(hidden, activation='tanh', return_sequences=True, dropout=dropout),\n",
        "        layers.SimpleRNN(hidden, activation='tanh', dropout=dropout),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "class JordanRNN(keras.Model):\n",
        "    def __init__(self, hidden):\n",
        "        super().__init__()\n",
        "        self.cell = layers.SimpleRNNCell(hidden, activation='tanh')\n",
        "        self.fc = layers.Dense(1)\n",
        "    def call(self, x, training=False):\n",
        "        h = tf.zeros((tf.shape(x)[0], self.cell.units))\n",
        "        y_prev = tf.zeros((tf.shape(x)[0], 1))\n",
        "        for xt in tf.unstack(x, axis=1):\n",
        "            inp = tf.concat([xt, y_prev], axis=1)\n",
        "            h, _ = self.cell(inp, [h])\n",
        "            y_prev = self.fc(h)\n",
        "        return y_prev\n",
        "\n",
        "\n",
        "full_df = load_and_prepare(FILE_PATH)\n",
        "df = full_df[(full_df[\"store\"] == STORE_ID) & (full_df[\"dept\"] == DEPT_ID)].copy()\n",
        "df = df.ffill().bfill()\n",
        "df = add_time_features(df)\n",
        "\n",
        "test_size = int(len(df)*TEST_FRAC)\n",
        "trainval_df, test_df = df.iloc[:-test_size], df.iloc[-test_size:]\n",
        "\n",
        "models = {\"Elman\": build_elman, \"Multi\": build_multi, \"Jordan\": JordanRNN}\n",
        "results = {}\n",
        "\n",
        "for model_name, builder in models.items():\n",
        "    print(f\"\\n=== Evaluating {model_name} ===\")\n",
        "    best_rmse, best_params = 1e9, None\n",
        "\n",
        "    for lookback, hidden, lr, dropout in product(*HYPERPARAMS.values()):\n",
        "        rmses = []\n",
        "\n",
        "        for train_idx, val_idx in get_cv_splits(len(trainval_df), CV_SPLITS, VAL_BLOCK_SIZE):\n",
        "            train_df = trainval_df.iloc[train_idx].copy()\n",
        "            val_df = trainval_df.iloc[val_idx].copy()\n",
        "\n",
        "            # encode categorical\n",
        "            dummies_train = pd.get_dummies(train_df[\"type\"], prefix=\"type\")\n",
        "            dummies_val = pd.get_dummies(val_df[\"type\"], prefix=\"type\").reindex(columns=dummies_train.columns, fill_value=0)\n",
        "            for dcol in dummies_train.columns:\n",
        "                train_df[dcol] = dummies_train[dcol]\n",
        "                val_df[dcol] = dummies_val[dcol]\n",
        "\n",
        "            features = NUMERIC_FEATURES + [\"isholiday\",\"week_sin\",\"week_cos\",\"time_idx\"] + list(dummies_train.columns)\n",
        "            features = [f for f in features if f in train_df.columns]\n",
        "\n",
        "            scaler_X, scaler_y = StandardScaler(), StandardScaler()\n",
        "            train_df[features] = scaler_X.fit_transform(train_df[features])\n",
        "            val_df[features] = scaler_X.transform(val_df[features])\n",
        "            train_df[[TARGET_COLUMN]] = scaler_y.fit_transform(train_df[[TARGET_COLUMN]])\n",
        "            val_df[[TARGET_COLUMN]] = scaler_y.transform(val_df[[TARGET_COLUMN]])\n",
        "\n",
        "            X_train, y_train = make_windows(train_df[features].values, train_df[[TARGET_COLUMN]].values, lookback)\n",
        "            val_context = pd.concat([train_df.tail(lookback), val_df])\n",
        "            X_val, y_val = make_windows(val_context[features].values, val_context[[TARGET_COLUMN]].values, lookback)\n",
        "            if X_train.size == 0 or X_val.size == 0: continue\n",
        "\n",
        "            shape = (lookback, X_train.shape[2])\n",
        "            if model_name == \"Jordan\":\n",
        "                model = builder(hidden)\n",
        "                model.build((None, *shape))\n",
        "            else:\n",
        "                model = builder(shape, hidden, dropout)\n",
        "\n",
        "            model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), loss=\"mse\")\n",
        "            cb = keras.callbacks.EarlyStopping(patience=PATIENCE, restore_best_weights=True)\n",
        "            model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0, callbacks=[cb])\n",
        "\n",
        "            preds = model.predict(X_val, verbose=0)\n",
        "            preds_inv = scaler_y.inverse_transform(preds)\n",
        "            true_inv = scaler_y.inverse_transform(y_val)\n",
        "            rmse = math.sqrt(mean_squared_error(true_inv, preds_inv))\n",
        "            rmses.append(rmse)\n",
        "\n",
        "        mean_rmse = np.mean(rmses)\n",
        "        print(f\"  Params (lookback={lookback}, hidden={hidden}) → RMSE={mean_rmse:.2f}\")\n",
        "        if mean_rmse < best_rmse:\n",
        "            best_rmse, best_params = mean_rmse, {'lookback': lookback, 'hidden': hidden, 'lr': lr, 'dropout': dropout}\n",
        "\n",
        "    # retrain on full train+val\n",
        "    print(f\"Best params for {model_name}: {best_params} (CV RMSE={best_rmse:.2f})\")\n",
        "\n",
        "    train_df, test_df_full = trainval_df.copy(), test_df.copy()\n",
        "    dummies_train = pd.get_dummies(train_df[\"type\"], prefix=\"type\")\n",
        "    dummies_test = pd.get_dummies(test_df_full[\"type\"], prefix=\"type\").reindex(columns=dummies_train.columns, fill_value=0)\n",
        "    for dcol in dummies_train.columns:\n",
        "        train_df[dcol] = dummies_train[dcol]\n",
        "        test_df_full[dcol] = dummies_test[dcol]\n",
        "\n",
        "    features = NUMERIC_FEATURES + [\"isholiday\",\"week_sin\",\"week_cos\",\"time_idx\"] + list(dummies_train.columns)\n",
        "    scaler_X, scaler_y = StandardScaler(), StandardScaler()\n",
        "    train_df[features] = scaler_X.fit_transform(train_df[features])\n",
        "    test_df_full[features] = scaler_X.transform(test_df_full[features])\n",
        "    train_df[[TARGET_COLUMN]] = scaler_y.fit_transform(train_df[[TARGET_COLUMN]])\n",
        "    test_df_full[[TARGET_COLUMN]] = scaler_y.transform(test_df_full[[TARGET_COLUMN]])\n",
        "\n",
        "    data_train = train_df[features].values\n",
        "    y_train = train_df[[TARGET_COLUMN]].values\n",
        "    data_test = pd.concat([train_df.tail(best_params[\"lookback\"]), test_df_full])\n",
        "    X_train, y_train = make_windows(data_train, y_train, best_params[\"lookback\"])\n",
        "    X_test, y_test = make_windows(data_test[features].values, data_test[[TARGET_COLUMN]].values, best_params[\"lookback\"])\n",
        "\n",
        "    shape = (best_params[\"lookback\"], X_train.shape[2])\n",
        "    if model_name == \"Jordan\":\n",
        "        final_model = JordanRNN(best_params[\"hidden\"])\n",
        "        final_model.build((None, *shape))\n",
        "    else:\n",
        "        final_model = builder(shape, best_params[\"hidden\"], best_params[\"dropout\"])\n",
        "\n",
        "    final_model.compile(optimizer=keras.optimizers.Adam(learning_rate=best_params[\"lr\"]), loss=\"mse\")\n",
        "    final_model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0)\n",
        "\n",
        "    preds = final_model.predict(X_test, verbose=0)\n",
        "    preds_inv = scaler_y.inverse_transform(preds)\n",
        "    true_inv = scaler_y.inverse_transform(y_test)\n",
        "    test_rmse = math.sqrt(mean_squared_error(true_inv, preds_inv))\n",
        "\n",
        "    results[model_name] = {\"params\": best_params, \"rmse\": test_rmse}\n",
        "    print(f\"Final Test RMSE for {model_name}: ${test_rmse:,.2f}\")\n",
        "\n",
        "\n",
        "print(\"\\n=== FINAL RESULTS ===\")\n",
        "for name, res in results.items():\n",
        "    print(f\"{name}: RMSE=${res['rmse']:.2f} → Params={res['params']}\")\n",
        "best = min(results.items(), key=lambda kv: kv[1][\"rmse\"])\n",
        "print(f\"\\nBest model overall: {best[0]} (RMSE=${best[1]['rmse']:.2f})\")\n"
      ],
      "metadata": {
        "id": "7w-jvg0qF5VV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = kagglehub.dataset_download(\"die9origephit/human-activity-recognition\")\n",
        "file_name = \"time_series_data_human_activities.csv\"\n",
        "file_path = os.path.join(path, file_name)\n",
        "df = pd.read_csv(file_path)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "MvdT-kNR1rur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, math, itertools, copy, pickle, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from statsmodels.tsa.stattools import adfuller, kpss\n",
        "\n",
        "\n",
        "DATASET_DIR = globals().get(\"path\", \".\")\n",
        "FILE_NAME = \"time_series_data_human_activities.csv\"\n",
        "FILE_PATH = os.path.join(DATASET_DIR, FILE_NAME)\n",
        "\n",
        "RESULTS_DIR = \"har_rnn_results\"\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "USER_COL = \"user\"\n",
        "ACTIVITY_COL = \"activity\"\n",
        "TIMESTAMP_COL = \"timestamp\"\n",
        "SENSOR_COLS = [\"x-axis\", \"y-axis\", \"z-axis\"]\n",
        "\n",
        "LOOKBACK_CANDIDATES = [64]\n",
        "HORIZON = 0\n",
        "STRIDE = 8\n",
        "MAX_SAMPLES = None\n",
        "BATCH_SIZE = 128\n",
        "N_EPOCHS = 10\n",
        "PATIENCE = 6\n",
        "N_CV_SPLITS = 3\n",
        "VAL_BLOCK_SIZE = 100000\n",
        "GAP = 0\n",
        "VERBOSE = True\n",
        "\n",
        "HYPERPARAM_GRID = {\n",
        "    'lookback': LOOKBACK_CANDIDATES,\n",
        "    'hidden': [64],\n",
        "    'lr': [1e-3],\n",
        "    'dropout': [0.2]\n",
        "}\n",
        "\n",
        "print(\"FILE_PATH:\", FILE_PATH)\n",
        "\n",
        "def try_parse_timestamp(s):\n",
        "    ts = pd.to_numeric(s, errors='coerce')\n",
        "    candidates = ['ns', 'us', 'ms', 's']\n",
        "    for unit in candidates:\n",
        "        try:\n",
        "            dt = pd.to_datetime(ts, unit=unit, errors='coerce')\n",
        "            median_year = pd.Series(dt).dt.year.dropna().median()\n",
        "            if not np.isnan(median_year) and 1980 <= median_year <= 2035:\n",
        "                return dt, unit\n",
        "        except Exception:\n",
        "            pass\n",
        "    try:\n",
        "        dt = pd.to_datetime(s, errors='coerce')\n",
        "        median_year = pd.Series(dt).dt.year.dropna().median()\n",
        "        if not np.isnan(median_year) and 1980 <= median_year <= 2035:\n",
        "            return dt, 'parsed'\n",
        "    except Exception:\n",
        "        pass\n",
        "    return None, None\n",
        "\n",
        "def read_har_csv(filepath, max_rows=None):\n",
        "    df = pd.read_csv(filepath)\n",
        "    df.columns = df.columns.str.strip().str.lower()\n",
        "    for c in [USER_COL.lower(), ACTIVITY_COL.lower(), TIMESTAMP_COL.lower()]:\n",
        "        if c not in df.columns:\n",
        "            raise ValueError(f\"Expected column '{c}' in CSV. Found: {df.columns.tolist()}\")\n",
        "    df = df.rename(columns={USER_COL.lower(): 'user', ACTIVITY_COL.lower(): 'activity', TIMESTAMP_COL.lower(): 'timestamp'})\n",
        "    dt, unit = try_parse_timestamp(df['timestamp'])\n",
        "    df['timestamp'] = dt\n",
        "    df = df.dropna(subset=['timestamp']).copy()\n",
        "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
        "    if max_rows is not None:\n",
        "        df = df.iloc[:max_rows].copy()\n",
        "    return df\n",
        "\n",
        "\n",
        "def stationarity_report_numeric(series, name='series'):\n",
        "    try:\n",
        "        a = adfuller(series.dropna(), autolag='AIC')\n",
        "        kst = kpss(series.dropna(), regression='c', nlags=\"auto\")\n",
        "        print(f\"ADF p={a[1]:.4f}; KPSS p={kst[1]:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(\"Stationarity test failed:\", e)\n",
        "\n",
        "def window_label_mode(labels):\n",
        "    c = Counter(labels)\n",
        "    most = c.most_common()\n",
        "    return most[0][0]\n",
        "\n",
        "def make_windows_classification(df, feature_cols, label_col, lookback, stride=1):\n",
        "    arr_X = df[feature_cols].values\n",
        "    arr_y = df[label_col].values\n",
        "    T = len(df)\n",
        "    starts = list(range(0, T - lookback + 1, stride))\n",
        "    Xs = []\n",
        "    Ys = []\n",
        "    for s in starts:\n",
        "        window_X = arr_X[s: s+lookback]\n",
        "        window_y = arr_y[s: s+lookback]\n",
        "        Xs.append(window_X)\n",
        "        Ys.append(window_label_mode(window_y))\n",
        "    if len(Xs)==0:\n",
        "        return np.empty((0, lookback, len(feature_cols))), np.empty((0,), dtype=np.int32)\n",
        "    return np.array(Xs, dtype=np.float32), np.array(Ys)\n",
        "\n",
        "def rolling_origin_splits_by_index(n_time, n_splits=N_CV_SPLITS, val_block_size=VAL_BLOCK_SIZE, gap=GAP, initial_train_size=None):\n",
        "    if initial_train_size is None:\n",
        "        initial_train_size = n_time - n_splits * val_block_size - gap * n_splits\n",
        "    for i in range(n_splits):\n",
        "        train_end = initial_train_size + i * val_block_size\n",
        "        val_start = train_end + gap\n",
        "        val_end = val_start + val_block_size\n",
        "        if val_end > n_time:\n",
        "            break\n",
        "        train_idx = np.arange(0, train_end)\n",
        "        val_idx = np.arange(val_start, val_end)\n",
        "        yield train_idx, val_idx\n",
        "\n",
        "def build_elman_classifier(input_shape, n_classes, hidden_size=64, dropout=0.0):\n",
        "    inp = keras.Input(shape=input_shape)\n",
        "    x = layers.SimpleRNN(hidden_size, activation='tanh', dropout=dropout, return_sequences=False)(inp)\n",
        "    out = layers.Dense(n_classes, activation='softmax')(x)\n",
        "    return keras.Model(inp, out)\n",
        "\n",
        "def build_multi_classifier(input_shape, n_classes, hidden_size=64, dropout=0.2):\n",
        "    inp = keras.Input(shape=input_shape)\n",
        "    x = layers.SimpleRNN(hidden_size, activation='tanh', dropout=dropout, return_sequences=True)(inp)\n",
        "    x = layers.SimpleRNN(hidden_size, activation='tanh', dropout=dropout, return_sequences=False)(x)\n",
        "    out = layers.Dense(n_classes, activation='softmax')(x)\n",
        "    return keras.Model(inp, out)\n",
        "\n",
        "# this is the big mans Jordan implementation. If you readijng this I couldnt get mione to work to be honest. My bad :)\n",
        "class JordanClassifier(keras.Model):\n",
        "    def __init__(self, input_size, n_classes, hidden_size=64, dropout=0.0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.rnn_cell = layers.SimpleRNNCell(hidden_size, activation='tanh')\n",
        "        self.dropout = layers.Dropout(dropout)\n",
        "        self.fc = layers.Dense(n_classes, activation='softmax')\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        batch = tf.shape(x)[0]\n",
        "        h = tf.zeros((batch, self.hidden_size))\n",
        "        y_prev = tf.zeros((batch, 1))  # previous *class* vector isn't available, we use previous logits as scalar 0 -> better: use zeros\n",
        "        # We'll feed previous output scalar (0) concatenated — this is a simplistic Jordan adaptation for classification\n",
        "        for t in range(x.shape[1]):\n",
        "            xt = x[:, t, :]\n",
        "            inp_t = tf.concat([xt, y_prev], axis=1)\n",
        "            out_cell, [h] = self.rnn_cell(inp_t, [h])\n",
        "            if training:\n",
        "                h = self.dropout(h, training=training)\n",
        "            # produce logits then reduce to a scalar to feed next step: take mean(logits) as scalar\n",
        "            logits = self.fc(h)  # (batch, n_classes)\n",
        "            # reduce to scalar in [-1,1] via tanh of mean\n",
        "            y_prev = tf.expand_dims(tf.tanh(tf.reduce_mean(logits, axis=1)), axis=1)\n",
        "        # final logits computed above (logits) -> but we need to return final class probs\n",
        "        return logits\n",
        "\n",
        "def compile_and_train(model, X_train, y_train, X_val, y_val, cfg):\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=cfg['lr']),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    es = keras.callbacks.EarlyStopping(patience=cfg['patience'], restore_best_weights=True, verbose=0)\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
        "                        epochs=cfg['epochs'], batch_size=cfg['batch_size'],\n",
        "                        callbacks=[es], verbose=cfg.get('verbose', 0))\n",
        "    return model, history\n",
        "\n",
        "def evaluate_classification(model, X, y, label_encoder, n_classes):\n",
        "    preds_prob = model.predict(X, batch_size=256)\n",
        "    preds = np.argmax(preds_prob, axis=1)\n",
        "    acc = accuracy_score(y, preds)\n",
        "    report = classification_report(y, preds, target_names=label_encoder.classes_, labels=np.arange(n_classes), zero_division=0)\n",
        "    return acc, report, preds\n",
        "\n",
        "\n",
        "def pipeline_har_classification(filepath):\n",
        "    df = read_har_csv(filepath, max_rows=MAX_SAMPLES)\n",
        "    print(\"Loaded rows:\", len(df))\n",
        "    sensor_cols = [c for c in df.columns if c in [s.lower() for s in SENSOR_COLS]]\n",
        "    if len(sensor_cols) == 0:\n",
        "        sensor_cols = SENSOR_COLS\n",
        "    le = LabelEncoder()\n",
        "    df['activity_enc'] = le.fit_transform(df['activity'].astype(str))\n",
        "    n_classes = len(le.classes_)\n",
        "    users = sorted(df['user'].unique())\n",
        "    user_map = {u: i for i, u in enumerate(users)}\n",
        "    df['user_enc'] = df['user'].map(user_map)\n",
        "    user_one_hot = False\n",
        "    if len(users) <= 128:\n",
        "        user_one_hot = True\n",
        "        user_dummies = pd.get_dummies(df['user_enc'], prefix='usr')\n",
        "        df = pd.concat([df.reset_index(drop=True), user_dummies.reset_index(drop=True)], axis=1)\n",
        "        user_cols = [c for c in df.columns if c.startswith('usr_')]\n",
        "    feature_cols = sensor_cols + user_cols\n",
        "\n",
        "    df = df.set_index('timestamp').sort_index()\n",
        "\n",
        "    df[feature_cols] = df[feature_cols].interpolate(method='time').ffill().bfill()\n",
        "    n = len(df)\n",
        "    test_n = max(1, int(0.10 * n))\n",
        "    trainval_df = df.iloc[:-test_n].copy()\n",
        "    test_df = df.iloc[-test_n:].copy()\n",
        "\n",
        "    n_time = len(trainval_df)\n",
        "    combos = list(itertools.product(HYPERPARAM_GRID['lookback'], HYPERPARAM_GRID['hidden'],\n",
        "                                    HYPERPARAM_GRID['lr'], HYPERPARAM_GRID['dropout']))\n",
        "    results = {}\n",
        "\n",
        "    for model_type in ['Elman', 'Jordan', 'Multi']:\n",
        "        print(\"\\n=== MODEL:\", model_type, \"===\")\n",
        "        combo_scores = []\n",
        "        for (lb, hid, lr, drop) in combos:\n",
        "            fold_accs = []\n",
        "            for (train_idx, val_idx) in rolling_origin_splits_by_index(n_time, n_splits=N_CV_SPLITS, val_block_size=VAL_BLOCK_SIZE, gap=GAP):\n",
        "                train_fold = trainval_df.iloc[train_idx].copy()\n",
        "                val_fold = trainval_df.iloc[val_idx].copy()\n",
        "                scaler = StandardScaler()\n",
        "                scaler.fit(train_fold[sensor_cols])\n",
        "                train_fold[sensor_cols] = scaler.transform(train_fold[sensor_cols])\n",
        "                val_fold[sensor_cols] = scaler.transform(val_fold[sensor_cols])\n",
        "                X_train, y_train = make_windows_classification(train_fold, feature_cols, 'activity_enc', lookback=lb, stride=STRIDE)\n",
        "                val_context = pd.concat([train_fold.tail(lb), val_fold])\n",
        "                X_val, y_val = make_windows_classification(val_context, feature_cols, 'activity_enc', lookback=lb, stride=STRIDE)\n",
        "                if X_train.shape[0]==0 or X_val.shape[0]==0:\n",
        "                    fold_accs.append(np.nan)\n",
        "                    continue\n",
        "\n",
        "                input_shape = (lb, X_train.shape[2])\n",
        "                if model_type == 'Elman':\n",
        "                    model = build_elman_classifier(input_shape, n_classes, hidden_size=hid, dropout=drop)\n",
        "                elif model_type == 'Multi':\n",
        "                    model = build_multi_classifier(input_shape, n_classes, hidden_size=hid, dropout=drop)\n",
        "                elif model_type == 'Jordan':\n",
        "                    model = JordanClassifier(input_size=X_train.shape[2], n_classes=n_classes, hidden_size=hid, dropout=drop)\n",
        "                    model.build((None, lb, X_train.shape[2]))\n",
        "                else:\n",
        "                    raise ValueError(\"Unknown model type\")\n",
        "\n",
        "                cfg = {'epochs': N_EPOCHS, 'lr': lr, 'batch_size': BATCH_SIZE, 'patience': PATIENCE, 'verbose': 0}\n",
        "                model, history = compile_and_train(model, X_train, y_train, X_val, y_val, cfg)\n",
        "                acc, rep, preds = evaluate_classification(model, X_val, y_val, le, n_classes)\n",
        "                fold_accs.append(acc)\n",
        "            fold_accs = [v for v in fold_accs if not np.isnan(v)]\n",
        "            mean_acc = np.mean(fold_accs) if len(fold_accs)>0 else np.nan\n",
        "            combo_scores.append({'lookback':lb, 'hidden':hid, 'lr':lr, 'dropout':drop, 'mean_cv_acc': mean_acc})\n",
        "\n",
        "        combo_scores_valid = [c for c in combo_scores if not np.isnan(c['mean_cv_acc'])]\n",
        "\n",
        "        best_combo = sorted(combo_scores_valid, key=lambda x: -x['mean_cv_acc'])[0]\n",
        "        print(\"Best combo:\", best_combo)\n",
        "        results[model_type] = {'best_combo': best_combo, 'combo_scores': combo_scores}\n",
        "\n",
        "        train_full = trainval_df.copy()\n",
        "        test_full = test_df.copy()\n",
        "        scaler = StandardScaler(); scaler.fit(train_full[sensor_cols])\n",
        "        train_full[sensor_cols] = scaler.transform(train_full[sensor_cols])\n",
        "        test_full[sensor_cols] = scaler.transform(test_full[sensor_cols])\n",
        "        lb = best_combo['lookback']; hid = best_combo['hidden']; lr = best_combo['lr']; drop = best_combo['dropout']\n",
        "        X_train_all, y_train_all = make_windows_classification(train_full, feature_cols, 'activity_enc', lookback=lb, stride=STRIDE)\n",
        "        test_context = pd.concat([train_full.tail(lb), test_full])\n",
        "        X_test, y_test = make_windows_classification(test_context, feature_cols, 'activity_enc', lookback=lb, stride=STRIDE)\n",
        "\n",
        "        if model_type == 'Elman':\n",
        "            final_model = build_elman_classifier((lb, X_train_all.shape[2]), n_classes, hidden_size=hid, dropout=drop)\n",
        "        elif model_type == 'Multi':\n",
        "            final_model = build_multi_classifier((lb, X_train_all.shape[2]), n_classes, hidden_size=hid, dropout=drop)\n",
        "        else:\n",
        "            final_model = JordanClassifier(input_size=X_train_all.shape[2], n_classes=n_classes, hidden_size=hid, dropout=drop)\n",
        "            final_model.build((None, lb, X_train_all.shape[2]))\n",
        "\n",
        "        cfg = {'epochs': N_EPOCHS, 'lr': lr, 'batch_size': BATCH_SIZE, 'patience': PATIENCE, 'verbose': 1}\n",
        "        final_model, history = compile_and_train(final_model, X_train_all, y_train_all, X_train_all[:0], y_train_all[:0], cfg)\n",
        "        acc_test, report_test, preds_test = evaluate_classification(final_model, X_test, y_test, le, n_classes)\n",
        "        print(f\" Final TEST accuracy for {model_type}: {acc_test:.4f}\")\n",
        "        print(\"Classification report (test):\")\n",
        "        print(report_test)\n",
        "\n",
        "        results[model_type].update({\n",
        "            'final_model': final_model,\n",
        "            'scaler': scaler,\n",
        "            'test_acc': acc_test,\n",
        "            'test_report': report_test,\n",
        "            'y_test': y_test,\n",
        "            'y_pred': preds_test\n",
        "        })\n",
        "\n",
        "\n",
        "    # summary\n",
        "    summary = {m: (results[m].get('test_acc', None)) for m in results}\n",
        "    print(\"Summary test accuracies:\", summary)\n",
        "\n",
        "start = time.time()\n",
        "res = pipeline_har_classification(FILE_PATH)"
      ],
      "metadata": {
        "id": "M727GrWc2i6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, math, itertools, copy, pickle, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from statsmodels.tsa.stattools import adfuller, kpss\n",
        "from math import sqrt\n",
        "from itertools import product\n",
        "\n",
        "\n",
        "\n",
        "path = kagglehub.dataset_download(\"krupalpatel07/microsoft-stock-data\")\n",
        "df = pd.read_csv(os.path.join(path, \"MSFT.csv\"))\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df = df.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "target = 'Close'\n",
        "features = ['Open', 'High', 'Low', 'Volume']\n",
        "\n",
        "df['ret'] = df[target].pct_change().fillna(0)\n",
        "df['ma_5'] = df[target].rolling(5, min_periods=1).mean()\n",
        "df['ma_10'] = df[target].rolling(10, min_periods=1).mean()\n",
        "df['time_idx'] = np.arange(len(df))\n",
        "features += ['ret','ma_5','ma_10','time_idx']\n",
        "\n",
        "test_frac = 0.1\n",
        "split = int(len(df)*(1-test_frac))\n",
        "train_df, test_df = df.iloc[:split], df.iloc[split:]\n",
        "\n",
        "scaler_X, scaler_y = StandardScaler(), StandardScaler()\n",
        "train_X = scaler_X.fit_transform(train_df[features])\n",
        "test_X = scaler_X.transform(test_df[features])\n",
        "train_y = scaler_y.fit_transform(train_df[[target]])\n",
        "test_y = scaler_y.transform(test_df[[target]])\n",
        "\n",
        "\n",
        "def make_windows(X, y, lookback=30):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X)-lookback):\n",
        "        Xs.append(X[i:i+lookback])\n",
        "        ys.append(y[i+lookback])\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "\n",
        "def build_elman(input_shape, hidden=32):\n",
        "    return keras.Sequential([\n",
        "        layers.SimpleRNN(hidden, activation='tanh', input_shape=input_shape),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "def build_multi(input_shape, hidden=32):\n",
        "    return keras.Sequential([\n",
        "        layers.SimpleRNN(hidden, activation='tanh', return_sequences=True, input_shape=input_shape),\n",
        "        layers.SimpleRNN(hidden, activation='tanh'),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "class JordanRNN(keras.Model):\n",
        "    def __init__(self, input_dim, hidden=32):\n",
        "        super().__init__()\n",
        "        self.cell = layers.SimpleRNNCell(hidden, activation='tanh')\n",
        "        self.out = layers.Dense(1)\n",
        "    def call(self, x, training=False):\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        h = tf.zeros((batch_size, self.cell.units))\n",
        "        y_prev = tf.zeros((batch_size, 1))\n",
        "        for xt in tf.unstack(x, axis=1):  # fixed symbolic loop\n",
        "            inp_t = tf.concat([xt, y_prev], axis=1)\n",
        "            h, _ = self.cell(inp_t, [h])\n",
        "            y_prev = self.out(h)\n",
        "        return y_prev\n",
        "\n",
        "def train_and_eval(model, X_train, y_train, X_test, y_test, scaler_y, epochs=10, batch_size=64):\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "    preds = model.predict(X_test, verbose=0)\n",
        "    preds_inv = scaler_y.inverse_transform(preds)\n",
        "    y_true_inv = scaler_y.inverse_transform(y_test)\n",
        "    rmse = sqrt(mean_squared_error(y_true_inv, preds_inv))\n",
        "    mae = mean_absolute_error(y_true_inv, preds_inv)\n",
        "    return rmse, mae\n",
        "\n",
        "lookbacks = [15, 30, 60]\n",
        "hiddens = [16, 32, 64]\n",
        "batches = [32, 64]\n",
        "\n",
        "results = {}\n",
        "input_dim = train_X.shape[1]\n",
        "\n",
        "for model_name in [\"Elman\", \"Multi\", \"Jordan\"]:\n",
        "    print(f\"\\n=== Searching {model_name} RNN ===\")\n",
        "    best_rmse, best_params = 1e9, None\n",
        "\n",
        "    for lookback, hidden, batch in product(lookbacks, hiddens, batches):\n",
        "        X_train, y_train = make_windows(train_X, train_y, lookback)\n",
        "        X_test, y_test = make_windows(test_X, test_y, lookback)\n",
        "        if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
        "            continue\n",
        "\n",
        "        input_shape = (lookback, X_train.shape[2])\n",
        "        if model_name == \"Elman\":\n",
        "            model = build_elman(input_shape, hidden)\n",
        "        elif model_name == \"Multi\":\n",
        "            model = build_multi(input_shape, hidden)\n",
        "        else:\n",
        "            model = JordanRNN(input_dim, hidden)\n",
        "            model.build((None, lookback, input_dim))\n",
        "\n",
        "        rmse, mae = train_and_eval(model, X_train, y_train, X_test, y_test, scaler_y,\n",
        "                                   epochs=10, batch_size=batch)\n",
        "        print(f\"Params (lb={lookback}, h={hidden}, bs={batch}) -> RMSE={rmse:.4f}, MAE={mae:.4f}\")\n",
        "\n",
        "        if rmse < best_rmse:\n",
        "            best_rmse = rmse\n",
        "            best_params = {'lookback': lookback, 'hidden': hidden, 'batch': batch, 'mae': mae}\n",
        "\n",
        "    results[model_name] = best_params\n",
        "\n",
        "print(\"\\n=== Best Results ===\")\n",
        "for m, r in results.items():\n",
        "    print(f\"{m}: RMSE={r['mae']:.4f}, Params={r}\")\n"
      ],
      "metadata": {
        "id": "s7qncwNzCmRi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}